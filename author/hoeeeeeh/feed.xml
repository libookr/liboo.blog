<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://blog.liboo.kr/author/hoeeeeeh/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://blog.liboo.kr/" rel="alternate" type="text/html" />
  <updated>2024-12-31T07:55:17+00:00</updated>
  <id>https://blog.liboo.kr/author/hoeeeeeh/feed.xml</id>

  
  
  

  
    <title type="html">Liboo.blog | </title>
  

  
    <subtitle>The professional publishing platform</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">🐬 서버를 docker swarm 으로 관리해보자</title>
      <link href="https://blog.liboo.kr/_%EC%84%9C%EB%B2%84%EB%A5%BC_docker_swarm_%EC%9C%BC%EB%A1%9C_%EA%B4%80%EB%A6%AC%ED%95%B4%EB%B3%B4%EC%9E%90" rel="alternate" type="text/html" title="🐬 서버를 docker swarm 으로 관리해보자" />
      <published>2024-12-04T05:41:00+00:00</published>
      <updated>2024-12-04T05:41:00+00:00</updated>
      <id>https://blog.liboo.kr/%F0%9F%90%AC_%EC%84%9C%EB%B2%84%EB%A5%BC_docker_swarm_%EC%9C%BC%EB%A1%9C_%EA%B4%80%EB%A6%AC%ED%95%B4%EB%B3%B4%EC%9E%90</id>
      <content type="html" xml:base="https://blog.liboo.kr/_%EC%84%9C%EB%B2%84%EB%A5%BC_docker_swarm_%EC%9C%BC%EB%A1%9C_%EA%B4%80%EB%A6%AC%ED%95%B4%EB%B3%B4%EC%9E%90">&lt;h1 id=&quot;기존-서버-운영-방식&quot;&gt;기존 서버 운영 방식&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-🐬_서버를_docker_swarm_으로_관리해보자.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;최초의 서버 설계&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;최초의 서버 설계에서는 NCP 서버 인스턴스 1대에 NGINX, 채팅 서버, API 서버, RTMP 서버가 각각 1개의 컨테이너씩 띄워져있는 구조였습니다.&lt;/p&gt;

&lt;p&gt;그러나 시간이 지나면서 각각의 서버를 수평 확장할 필요성을 느끼고 서버를 수평 확장 할 수 있도록 설계를 수정하고 보니 컨테이너를 어떻게 관리하고, 또 ncp 서버 인스턴스를 어떻게 추가/삭제 하면서 관리해야할지에 대한 어려움이 생겼습니다.&lt;/p&gt;

&lt;h2 id=&quot;docker-swarm&quot;&gt;Docker Swarm&lt;/h2&gt;

&lt;p&gt;도커 스웜은 쿠버네티스와 함께 이야기되는 대표적인 컨테이너 오케스트레이션 도구입니다.  이름에서부터 알 수 있듯이 도커 엔진을 그대로 쓰기 때문에 간단한 설정만으로도 클러스터와 서비스들을 관리할 수 있습니다.&lt;/p&gt;

&lt;p&gt;엔터프라이즈 레벨의 운영을 한다면 쿠버네티스가 조금 더 좋았겠지만 아직까지는 저희 프로젝트에서 네트워킹이나 Volume 등을 고려해봤을 때, 간편하게 설정할 수 있는 도커 스웜을 사용하는 것이 좋다고 생각했습니다.&lt;/p&gt;

&lt;h2 id=&quot;클러스터링&quot;&gt;클러스터링&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-🐬_서버를_docker_swarm_으로_관리해보자.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-🐬_서버를_docker_swarm_으로_관리해보자.md/2.png&quot; alt=&quot;2&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;저희 팀은 기존에 메인 production 서버 1대, 테스트용 development 서버 1대로 총 2대 가동 하고 있었습니다.&lt;/p&gt;

&lt;p&gt;그리고 개발이 어느정도 마무리 되는 시점인 6주차에 접어들면서 테스트용 development 서버의 활용성이 점점 떨어졌고, 부하 테스트나 데모 용도로 production 서버에 부하가 점점 늘어났습니다.&lt;/p&gt;

&lt;p&gt;그래서 기존의 test 서버를 production 서버와 클러스터링 하면서 도커 컨테이너 기반으로 서비스를 운영하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-🐬_서버를_docker_swarm_으로_관리해보자.md/3.png&quot; alt=&quot;3&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-🐬_서버를_docker_swarm_으로_관리해보자.md/4.png&quot; alt=&quot;4&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;위의 이미지에서 보시다시피 main, test 2개의 서버에 chat-server (replica 3), main-server, rtmp-server (replica 3), nginx 컨테이너를 나누어 실행하고 있습니다.&lt;/p&gt;

&lt;p&gt;Docker Swarm 은 아쉽게도 쿠버네티스처럼 컨테이너에 대한 메트릭을 자동으로 수집해서 오토 스케일링을 하는 작업을 공식적으로 제공하지는 않습니다. 따라서 현재는 수동적으로 ncp 인스턴스를 클러스터링하고 서비스의 scale 을 늘리고 줄여야 합니다.&lt;/p&gt;

&lt;p&gt;하지만 메트릭 수집과 자동 스케일링을 할 수 있도록 스크립트를 작성하는 방식으로 리팩토링 하여 오토 스케일링을 구현할 예정입니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      

      
        <summary type="html">기존 서버 운영 방식</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">채팅 서버에 Redis 를 쓴 이유</title>
      <link href="https://blog.liboo.kr/%EC%B1%84%ED%8C%85_%EC%84%9C%EB%B2%84%EC%97%90_Redis_%EB%A5%BC_%EC%93%B4_%EC%9D%B4%EC%9C%A0" rel="alternate" type="text/html" title="채팅 서버에 Redis 를 쓴 이유" />
      <published>2024-12-04T05:36:00+00:00</published>
      <updated>2024-12-04T05:36:00+00:00</updated>
      <id>https://blog.liboo.kr/%EC%B1%84%ED%8C%85_%EC%84%9C%EB%B2%84%EC%97%90_Redis_%EB%A5%BC_%EC%93%B4_%EC%9D%B4%EC%9C%A0</id>
      <content type="html" xml:base="https://blog.liboo.kr/%EC%B1%84%ED%8C%85_%EC%84%9C%EB%B2%84%EC%97%90_Redis_%EB%A5%BC_%EC%93%B4_%EC%9D%B4%EC%9C%A0">&lt;h1 id=&quot;초기-설계&quot;&gt;초기 설계&lt;/h1&gt;

&lt;p&gt;초기의 채팅 서버 설계는 굉장히 직관적인 설계를 가졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-채팅_서버에_Redis_를_쓴_이유.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;초기 채팅 서버 설계&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://socket.io/&quot;&gt;socket.io&lt;/a&gt; 와 nest.js 를 활용해서 채팅 서버를 작성했고, 1대의 채팅 서버가 유저의 모든 메세지 이벤트를 받아, 같은 방에 있는 모든 클라이언트에게 메세지 이벤트를 emit 하는 구조였습니다.&lt;/p&gt;

&lt;h1 id=&quot;초기-설계에서의-단순-수평-확장&quot;&gt;초기 설계에서의 단순 수평 확장&lt;/h1&gt;

&lt;p&gt;초기의 단순한 설계에서 부하가 없을 때는 큰 문제를 느끼지 못했습니다. 그러나 과도한 부하가 걸리는 상황에는 채팅 서버의 확장이 필요해졌습니다.&lt;/p&gt;

&lt;p&gt;프론트엔드 분들이 SharedWorker 소켓을 도입함으로써 서버의 부하를 줄였다면, 서버는 수평적 확장을 통해서 부하를 컨트롤 하는 것을 시도해봤습니다.&lt;/p&gt;

&lt;p&gt;하지만 단순히 새로운 채팅 서버 컨테이너를 하나 더 띄워서 서비스를 사용해보면 문제가 발생합니다.&lt;/p&gt;

&lt;p&gt;우선 유저들간 채팅의 동기화가 이루어지지 않습니다. 누군가 보낸 채팅을 다른 누군가는 보기도 하고, 못 보기도 합니다.
정확히는 같은 채팅 서버에 있는 유저는 볼 수 있고, 다른 채팅 서버에 있는 유저는 볼 수 없습니다.&lt;/p&gt;

&lt;p&gt;또한 서버에서도 유저를 차단하려고 했을 때, 어느 서버에서 데이터를 가져와야 할 지 모르기 때문에 모든 서버에 해당 유저의 정보가 존재하는지 요청을 보내야 합니다.&lt;/p&gt;

&lt;p&gt;이처럼 채팅 서버를 수평 확장했을 때, 채팅 서버간 데이터 공유를 할 수 있어야 한다는 문제점이 발생하게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-채팅_서버에_Redis_를_쓴_이유.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;단순하게 수평 확장한 구조&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;수평-확장을-고려한-구조-재설계&quot;&gt;수평 확장을 고려한 구조 재설계&lt;/h1&gt;

&lt;p&gt;저희는 이런 문제를 해결하기 위해서 redis pub/sub 구조를 활용했습니다.
채팅 서버 A 의 ‘liboo’ 라는 room 으로 메세지가 온다면, 채팅 서버 A 는 redis pub/sub 구조를 통해서 다른 채팅 서버 B/C 로도 메세지를 전파할 수 있습니다.
채팅 서버 B, C 는 레디스로부터 받은 이벤트를 기반으로 해당 채팅 서버에 있는 클라이언트에게 메세지를 emit 합니다.&lt;/p&gt;

&lt;p&gt;또한, 채팅 서버를 수평 확장하더라도 redis 인스턴스가 하나라면 같은 문제가 반복된다고 생각했습니다.
따라서 redis 인스턴스도 확장이 가능하도록 redis-cluster 로 구축했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-채팅_서버에_Redis_를_쓴_이유.md/2.png&quot; alt=&quot;2&quot; /&gt;&lt;em&gt;redis pub/sub 을 활용한 구조 재설계&lt;/em&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      

      
        <summary type="html">초기 설계</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">확장성을 고려한 초반 서버 설계</title>
      <link href="https://blog.liboo.kr/%ED%99%95%EC%9E%A5%EC%84%B1%EC%9D%84_%EA%B3%A0%EB%A0%A4%ED%95%9C_%EC%B4%88%EB%B0%98_%EC%84%9C%EB%B2%84_%EC%84%A4%EA%B3%84" rel="alternate" type="text/html" title="확장성을 고려한 초반 서버 설계" />
      <published>2024-12-04T05:04:00+00:00</published>
      <updated>2024-12-04T05:04:00+00:00</updated>
      <id>https://blog.liboo.kr/%ED%99%95%EC%9E%A5%EC%84%B1%EC%9D%84_%EA%B3%A0%EB%A0%A4%ED%95%9C_%EC%B4%88%EB%B0%98_%EC%84%9C%EB%B2%84_%EC%84%A4%EA%B3%84</id>
      <content type="html" xml:base="https://blog.liboo.kr/%ED%99%95%EC%9E%A5%EC%84%B1%EC%9D%84_%EA%B3%A0%EB%A0%A4%ED%95%9C_%EC%B4%88%EB%B0%98_%EC%84%9C%EB%B2%84_%EC%84%A4%EA%B3%84">&lt;h1 id=&quot;확장성&quot;&gt;확장성&lt;/h1&gt;

&lt;p&gt;Liboo 프로젝트의 처음 구조를 설계할때는 아래 그림과 같은 구조였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-확장성을_고려한_초반_서버_설계.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;저희 서버는 네이버 부스트캠프 최종 발표 시점에 생기는 부하를 감당할 수 있을 정도로 설계를 해보는 것이 목표였습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그림이 다소 난잡하지만 주요한 부분은 RTMP 서버, 메인 (API) 서버, 그림에는 없지만 채팅 서버를 전부 분리했다는 점입니다.&lt;/p&gt;

&lt;p&gt;이렇게 분리했던 이유는 RTMP 서버는 RTMP 스트림 데이터를 HLS 세그먼트와 index 파일로 트랜스파일링 하는, CPU intensive 한 작업을 많이 할 거라고 생각했기 때문입니다.&lt;/p&gt;

&lt;p&gt;그에 반해 API 서버는 클라이언트의 요청을 처리하면서 메모리와 DB I/O 에 더 집중적이고,&lt;/p&gt;

&lt;p&gt;채팅 서버는 사용자 수가 매우 많아진다면 동시에 처리 하는데 CPU 를 많이 쓸 수 있겠지만 기본적으로 메모리와 네트워크 처리량이 조금 더 집중적이라고 생각했습니다.&lt;/p&gt;

&lt;p&gt;RTMP, API, 채팅 서버가 하나의 서버로 관리된다면 RTMP 스트림 처리 작업을 위해서 CPU 사용량이 과도하게 높아지면 서버를 스케일링할 때 API 와 채팅 서버도 같이 늘어나야 합니다.&lt;/p&gt;

&lt;p&gt;저희는 서버를 각각의 역할에 맞게, 집중적으로 사용하는 부분에 맞게 분리를 해서 서버 스케일링이 효율적으로 이루어질 수 있도록, 확장성을 고려해보는 것이 이번 프로젝트의 가장 큰 목표였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-12-04-확장성을_고려한_초반_서버_설계.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;전체적인 서버의 아키텍처 구조는 위와 같습니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      

      
        <summary type="html">확장성</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">4.5.1 버전에서 ts, yarn.lock 에러</title>
      <link href="https://blog.liboo.kr/4.5.1_%EB%B2%84%EC%A0%84%EC%97%90%EC%84%9C_ts,_yarn.lock_%EC%97%90%EB%9F%AC" rel="alternate" type="text/html" title="4.5.1 버전에서 ts, yarn.lock 에러" />
      <published>2024-11-28T11:56:00+00:00</published>
      <updated>2024-11-28T11:56:00+00:00</updated>
      <id>https://blog.liboo.kr/4.5.1_%EB%B2%84%EC%A0%84%EC%97%90%EC%84%9C_ts,_yarn.lock_%EC%97%90%EB%9F%AC</id>
      <content type="html" xml:base="https://blog.liboo.kr/4.5.1_%EB%B2%84%EC%A0%84%EC%97%90%EC%84%9C_ts,_yarn.lock_%EC%97%90%EB%9F%AC">&lt;h2 id=&quot;문제-상황&quot;&gt;🚨 문제 상황&lt;/h2&gt;

&lt;p&gt;백엔드 작업 내용과 프론트 작업 내용을 합치는 과정에서 yarn install로 의존성 패키지들을 설치하면, @nestjs가 비정상적으로 동작하는 문제를 발견했습니다&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-11-28-4.5.1_버전에서_ts,_yarn.lock_에러.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;해결-과정&quot;&gt;🏃 해결 과정&lt;/h2&gt;

&lt;p&gt;yarn set version berry를 통해 레포지토리의 yarn 버전을 4.5.1에서 4.5.3으로 업데이트하니까 버그가 해결되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-11-28-4.5.1_버전에서_ts,_yarn.lock_에러.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;문제-해결&quot;&gt;✅ 문제 해결&lt;/h2&gt;

&lt;p&gt;그런데 아직까지 정확하게 왜 4.5.1 버전에서 에러가 발생하는 지와 4.5.3 버전으로 업데이트 했을 때 오류가 해결되는지 원인을 찾지 못했습니다.&lt;/p&gt;

&lt;p&gt;추후에 yarn에 대해 더 학습해보면서 원인에 대해 학습해볼 예정입니다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;김준서&quot;, &quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      

      
        <summary type="html">🚨 문제 상황</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Github Package 로 라이브러리 배포하기</title>
      <link href="https://blog.liboo.kr/Github_Package_%EB%A1%9C_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC_%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0" rel="alternate" type="text/html" title="Github Package 로 라이브러리 배포하기" />
      <published>2024-11-11T02:30:00+00:00</published>
      <updated>2024-11-11T02:30:00+00:00</updated>
      <id>https://blog.liboo.kr/Github_Package_%EB%A1%9C_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC_%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0</id>
      <content type="html" xml:base="https://blog.liboo.kr/Github_Package_%EB%A1%9C_%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC_%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0">&lt;p&gt;프로젝트를 진행하면서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;node-media-server&lt;/code&gt; 라이브러리를 사용하기로 결정해서 소스 코드를 천천히 뜯어보니 우리 서비스에 맞게끔 코드를 조금 수정할 필요가 있었다.&lt;/p&gt;

&lt;p&gt;처음에는 node-media-server 소스 코드를 전부 들고와서 liboo 레포지토리에 그대로 넣은 후 코드를 수정했었는데, 경우에 따라서 누군가 코드를 읽다가 ‘이 파일들은 뭐지?’ 같은 의구심을 품을 수 있을 것 같았다.&lt;/p&gt;

&lt;p&gt;따라서 서비스에 맞게 수정한 node-media-server 를 분리하면서 다른 라이브러리를 사용하는 것 처럼 import 할 수 있도록 Github Package 를 활용해보려고 한다.&lt;/p&gt;

&lt;h2 id=&quot;github-package--npm&quot;&gt;Github Package , npm&lt;/h2&gt;

&lt;p&gt;Github Package 와 npm 은 모두 패키지 저장소의 역할을 하지만 npm 은 private 이 유료, Github Package 는 무료라는 차이가 있다. 또한 Github Package 는 Github 의 기능이기 때문에 다른 기능, 예를 들어 Github Action 같은 기능과의 연동이 쉽다.&lt;/p&gt;

&lt;p&gt;liboo 의 node-media-server 라이브러리는 private 이든, public 이든 큰 상관은 없지만 추후에 Github Action 을 활용해서 배포하는 것이 편할 것 같다는 생각에 Github Package 를 선택했다.&lt;/p&gt;

&lt;h2 id=&quot;github-package-publish&quot;&gt;Github Package publish&lt;/h2&gt;

&lt;h3 id=&quot;fork&quot;&gt;Fork&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-11-11-Github_Package_로_라이브러리_배포하기.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;일단 사용하고자 하는 라이브러리인 Node-Media-Server 를 fork 해왔다.&lt;/p&gt;

&lt;p&gt;MIT 라이센스는 수정, 배포, 복제 등을 자유롭게 허용하는 관대한 라이센스라서 마음대로 fork 해와서 사용해도 괜찮지만 원저작자의 고지는 해야한다.&lt;/p&gt;

&lt;h3 id=&quot;packagejson&quot;&gt;Package.json&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;json
  &quot;name&quot;: &quot;@hoeeeeeh/node-media-server&quot;,
  &quot;publishConfig&quot;: {
    &quot;registry&quot;: &quot;https://npm.pkg.github.com/&quot;
  },
  &quot;version&quot;: &quot;2.7.27&quot;,
  &quot;description&quot;: &quot;A Node.js implementation of RTMP Server for naver boostcamp project : liboo&quot;,
  &quot;bin&quot;: &quot;bin/app.js&quot;,
  &quot;main&quot;: &quot;src/node_media_server.js&quot;,
  &quot;types&quot;: &quot;src/node_media_server.d.ts&quot;,

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;name: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@hoeeeeeh&lt;/code&gt; 처럼 앞에 자신의 Github ID 로 구분자를 주면 된다.&lt;/li&gt;
  &lt;li&gt;publishConfig: npm 이 아닌 Github Package 로 지정해야하기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;registry&quot;: &quot;https://npm.pkg.github.com/&quot;&lt;/code&gt; 로 지정한다.&lt;/li&gt;
  &lt;li&gt;version: 버전은 아무렇게나 지정해도 되는데, 만약 2.7.27 로 배포한 후에 수정 후 다시 배포한다면 2.7.27 이 아닌 다른 버전이어야 한다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main, types&lt;/code&gt; : 기존의 node-media-server 는 js 로 작성된 라이브러리이다. 그러나 liboo 의 서버는 ts 로 작성할 예정이기 때문에 자바스크립트로 작성한 것들의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;타입&lt;/code&gt; 에 대한 선언이 필요하다.&lt;/p&gt;

    &lt;p&gt;보통은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@types/{package_name}&lt;/code&gt;  으로 타입을 내려받을 수 있겠지만 Github Package 에는 npm 의 @types 같은 네임스페이스가 없다.&lt;/p&gt;

    &lt;p&gt;그래서 우리는 package.json 에 “types” 를 만들고, 타입에 대한 선언을 담고 있는 d.ts 파일을 지정해줘야한다.&lt;/p&gt;

    &lt;p&gt;다행히도 node-media-server 는 이미 @types 에 선언되어있던 것이 있다. 그래서 그 파일을 들고와서 liboo 에 맞게끔 조금 수정한 뒤에, types: ….d.ts 로 걸어주면 된다.&lt;/p&gt;

    &lt;p&gt;타입스크립트는 이름이 같은 js 와 d.ts 인 경우에는 자동으로 타입으로 생각해서 읽는다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;publish&quot;&gt;Publish&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;json
// 프로젝트 root 에 .npmrc 혹은 .yarnrc.yml
npmScopes:
  hoeeeeeh:
    npmRegistries:
      &quot;https://npm.pkg.github.com/&quot;:

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;스코프별로 레지스트리를 다르게 지정해줄 수 있다.&lt;/p&gt;

&lt;p&gt;우리는 @hoeeeeeh/node-media-server 를 배포하는 것이기 때문에, hoeeeeeh 의 레지스트리를 Github Package 로 지정해준 것이다.&lt;/p&gt;

&lt;p&gt;만약 스코프를 지정하지않고 npm 레지스트리를 Github Package 로 바꿔버리면 평범한 라이브러리를 설치하려고 해도 github package 로 접근해서 찾기 때문에 문제가 발생한다.&lt;/p&gt;

&lt;p&gt;이후에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;npm publish&lt;/code&gt; 혹은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yarn publish&lt;/code&gt; 를 통해 배포해주면 된다.&lt;/p&gt;

&lt;h3 id=&quot;install&quot;&gt;Install&lt;/h3&gt;

&lt;p&gt;Github Package 는 npm 과는 다르게 인증 절차가 있어야만 install 을 할 수 있다.&lt;/p&gt;

&lt;p&gt;배포한 패키지를 사용할 프로젝트에서도 아래와 같은 셋팅을 해야한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;json
// .yarnrc.yml
npmScopes:
  hoeeeeeh:
    npmRegistryServer: &quot;https://npm.pkg.github.com/&quot;
    npmAuthToken: ${npmAuthToken}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그런데 .yarnrc.yml 에는 환경변수를 활용할 수가 없다. yarn v4 에서나 가능하다고 해서 지금 당장 환경변수를 활용하려면 아래와 같은 방법을 사용해야한다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;javascript
// .yarn/plugin-env-npm.js
module.exports = {
  name: &apos;plugin-env-npm&apos;,
  factory: require =&amp;gt; ({
    hooks: {
      async getNpmAuthenticationHeader(currentHeader, registry, {ident}){
        // only trigger for specific scope
        if (!ident || ident.scope !== &apos;hoeeeeeh&apos;) {
          return currentHeader
        }

        // try getting token from process.env
        let bufEnv = process.env.BUF_REGISTRY_TOKEN
        // alternatively, try to find it in .env
        if (!bufEnv) {
          const fs = require(&apos;fs/promises&apos;)
          const fileContent = await fs.readFile(&apos;../.env&apos;, &apos;utf8&apos;)
          const rows = fileContent.split(/\r?\n/)
          for (const row of rows) {
            const [key, value] = row.split(&apos;:&apos;, 2)
            if (key.trim() === &apos;GITHUB_REGISTRY_TOKEN&apos;) {
              bufEnv = value.trim()
            }
          }
        }

        if (bufEnv) {
          return `${bufEnv}`
        }
        return currentHeader
      },
    },
  }),
}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;프로젝트 루트에 있는 .yarn 폴더에 plugin-env-npm.js 라는 파일을 하나 작성하자.&lt;/p&gt;

&lt;p&gt;내용은 그냥 .env 파일을 불러와서 환경변수를 읽어오는 내용이다.&lt;/p&gt;

&lt;p&gt;이후에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.yarnrc.yml&lt;/code&gt; 에는 아래와 같이 작성하면 된다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;javascript
npmScopes:
  hoeeeeeh:
    npmRegistryServer: &quot;https://npm.pkg.github.com/&quot;

plugins:
  - ./.yarn/plugin-env-npm.js

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;프로젝트 루트의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.env&lt;/code&gt; 도 작성해주자.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;javascript
GITHUB_REGISTRY_TOKEN: ${TOKEN_VALUE}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      
        <category term="github-package" />
      
        <category term="yarn" />
      

      
        <summary type="html">프로젝트를 진행하면서 node-media-server 라이브러리를 사용하기로 결정해서 소스 코드를 천천히 뜯어보니 우리 서비스에 맞게끔 코드를 조금 수정할 필요가 있었다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">[docker] yarn-berry workspace 를 찾지 못하는 오류</title>
      <link href="https://blog.liboo.kr/docker-_yarn-berry_workspace_%EB%A5%BC_%EC%B0%BE%EC%A7%80_%EB%AA%BB%ED%95%98%EB%8A%94_%EC%98%A4%EB%A5%98" rel="alternate" type="text/html" title="[docker] yarn-berry workspace 를 찾지 못하는 오류" />
      <published>2024-11-07T09:56:00+00:00</published>
      <updated>2024-11-07T09:56:00+00:00</updated>
      <id>https://blog.liboo.kr/%5Bdocker%5D_yarn-berry_workspace_%EB%A5%BC_%EC%B0%BE%EC%A7%80_%EB%AA%BB%ED%95%98%EB%8A%94_%EC%98%A4%EB%A5%98</id>
      <content type="html" xml:base="https://blog.liboo.kr/docker-_yarn-berry_workspace_%EB%A5%BC_%EC%B0%BE%EC%A7%80_%EB%AA%BB%ED%95%98%EB%8A%94_%EC%98%A4%EB%A5%98">&lt;p&gt;패키지매니저를 yarn-berry 로 쓰고 dockerfile 을 아래와 같이 작성했는데&lt;/p&gt;

&lt;p&gt;yarn install 을 하면서&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Error: nodeMediaServer@workspace:^: Workspace not found (nodeMediaServer@workspace:^)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;라는 오류가 발생했다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yaml
FROM node:20-alpine

RUN corepack enable

# Setting working directory
WORKDIR /app

# Copying package.json and package-lock.json
COPY package*.json ./

# Installing dependencies
RUN yarn install

# Install FFmpeg. This is needed to convert the video to HLS
RUN apk add --no-cache ffmpeg

# /usr/bin/ffmpeg is the default path for ffmpeg, copy it to /app
RUN cp /usr/bin/ffmpeg ./

# Copying all the files
COPY . .

# Exposing ports
EXPOSE 8000
EXPOSE 1935

# Running the app
CMD [&quot;yarn&quot;, &quot;start&quot;]


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;basic
[+] Building 3.9s (9/12)                                                                                                                                                                                            docker:desktop-linux
 =&amp;gt; [media-server internal] load build definition from dockerfile                                                                                                                                                                   0.0s
 =&amp;gt; =&amp;gt; transferring dockerfile: 573B                                                                                                                                                                                                0.0s
 =&amp;gt; [media-server internal] load .dockerignore                                                                                                                                                                                      0.0s
 =&amp;gt; =&amp;gt; transferring context: 2B                                                                                                                                                                                                     0.0s
 =&amp;gt; [media-server internal] load metadata for docker.io/library/node:20-alpine                                                                                                                                                      1.0s
 =&amp;gt; [media-server 1/8] FROM docker.io/library/node:20-alpine@sha256:c13b26e7e602ef2f1074aef304ce6e9b7dd284c419b35d89fcf3cc8e44a8def9                                                                                                0.0s
 =&amp;gt; [media-server internal] load build context                                                                                                                                                                                      0.0s
 =&amp;gt; =&amp;gt; transferring context: 6.16kB                                                                                                                                                                                                 0.0s
 =&amp;gt; CACHED [media-server 2/8] RUN corepack enable                                                                                                                                                                                   0.0s
 =&amp;gt; CACHED [media-server 3/8] WORKDIR /app                                                                                                                                                                                          0.0s
 =&amp;gt; [media-server 4/8] COPY package*.json ./                                                                                                                                                                                        0.0s
 =&amp;gt; ERROR [media-server 5/8] RUN yarn install                                                                                                                                                                                       2.9s
------
 &amp;gt; [media-server 5/8] RUN yarn install:
0.483 ! Corepack is about to download https://repo.yarnpkg.com/4.5.1/packages/yarnpkg-cli/bin/yarn.js
0.912 ➤ YN0000: · Yarn 4.5.1
0.923 ➤ YN0000: ┌ Resolution step
1.030 ➤ YN0001: │ Error: nodeMediaServer@workspace:^: Workspace not found (nodeMediaServer@workspace:^)
1.030     at t.getWorkspaceByDescriptor (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:210:3520)
1.030     at t.getCandidates (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:140:117086)
1.030     at Pg.getCandidates (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:141:1311)
1.030     at Pg.getCandidates (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:141:1311)
1.030     at /root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:210:8420
1.030     at Jm (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:140:53873)
1.030     at gt (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:210:8400)
1.030     at async Promise.allSettled (index 1)
1.030     at async Yc (/root/.cache/node/corepack/v1/yarn/4.5.1/yarn.js:140:53201)
1.030 ➤ YN0000: └ Completed
1.030 ➤ YN0000: · Failed with errors in 0s 119ms
------
failed to solve: process &quot;/bin/sh -c yarn install&quot; did not complete successfully: exit code: 1


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이유는 COPY 와 yarn install 의 순서 때문이었다.&lt;/p&gt;

&lt;p&gt;올바른 도커파일&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yaml
FROM node:20-alpine

RUN corepack enable

# Setting working directory
WORKDIR /app

# Copying package.json and package-lock.json
COPY package*.json ./

# Install FFmpeg. This is needed to convert the video to HLS
RUN apk add --no-cache ffmpeg

# /usr/bin/ffmpeg is the default path for ffmpeg, copy it to /app
RUN cp /usr/bin/ffmpeg ./

# Copying all the files
COPY . ./

# Installing dependencies
RUN yarn install

# Exposing ports
EXPOSE 8000
EXPOSE 1935

# Running the app
CMD [&quot;yarn&quot;, &quot;start&quot;]


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;COPY . . 를 해야 yarn install 을 하면서 워크스페이스를 찾을텐데, copy 전에 yarn install 을 먼저 해서 워크스페이스를 찾을 수 없었던 것이다.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;hoeeeeeh&quot;]</name>
        
        
      </author>

      

      
        <category term="docker" />
      

      
        <summary type="html">패키지매니저를 yarn-berry 로 쓰고 dockerfile 을 아래와 같이 작성했는데</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">동영상 스트리밍 처리 프로토콜을 알아보자</title>
      <link href="https://blog.liboo.kr/%EB%8F%99%EC%98%81%EC%83%81_%EC%8A%A4%ED%8A%B8%EB%A6%AC%EB%B0%8D_%EC%B2%98%EB%A6%AC_%ED%94%84%EB%A1%9C%ED%86%A0%EC%BD%9C%EC%9D%84_%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90" rel="alternate" type="text/html" title="동영상 스트리밍 처리 프로토콜을 알아보자" />
      <published>2024-10-28T02:40:00+00:00</published>
      <updated>2024-10-28T02:40:00+00:00</updated>
      <id>https://blog.liboo.kr/%EB%8F%99%EC%98%81%EC%83%81_%EC%8A%A4%ED%8A%B8%EB%A6%AC%EB%B0%8D_%EC%B2%98%EB%A6%AC_%ED%94%84%EB%A1%9C%ED%86%A0%EC%BD%9C%EC%9D%84_%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90</id>
      <content type="html" xml:base="https://blog.liboo.kr/%EB%8F%99%EC%98%81%EC%83%81_%EC%8A%A4%ED%8A%B8%EB%A6%AC%EB%B0%8D_%EC%B2%98%EB%A6%AC_%ED%94%84%EB%A1%9C%ED%86%A0%EC%BD%9C%EC%9D%84_%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90">&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;목차&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;학습-이유&quot;&gt;🤔 학습 이유&lt;/h1&gt;

&lt;p&gt;서비스의 레퍼런스로 Zoom과 치지직 등 여러 스트리밍 서비스의 동작원리를 조사하던 중, 각 서비스별로 서로 다른 프로토콜을 사용하는 것을 발견할 수 있었다.&lt;/p&gt;

&lt;p&gt;영상 송출에 사용되는 프로토콜을 조사하고 우리 서비스에 적합한 프로토콜을 선정하고자 한다!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;동영상 스트리밍 처리 프로토콜
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;HLS&lt;/strong&gt; &lt;strong&gt;- 지연시간이 높지만 호환성이 좋음 (치지직) - 민지, 영길&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;왜 다른 프로토콜에 비해 지연시간이 길까?&lt;/li&gt;
          &lt;li&gt;어떻게 지연시간을 낮출 수 있을까?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;WebRTC - 지연시간이 낮지만 P2P에 적합 (구글미트) - 창현&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;RTMP - 지연시간 낮음 (트위치) - 준서, 지수&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;Dynamic Adapltive Streaming over HTTP&lt;/li&gt;
      &lt;li&gt;SRT&lt;/li&gt;
      &lt;li&gt;RTSP&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1️⃣rtsp-real-time-streaming-protocol&quot;&gt;1️⃣ RTSP &lt;strong&gt;(Real Time Streaming Protocol)&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;스트리밍의 시작이라고 말할 수 있다. 1996년 등장하였으며 RTSP가 등장 전 영상, 음악 등 멀티미디어 정보를 완전히 다운로드한 후 시청할 수 있었음.&lt;/p&gt;

&lt;p&gt;다만 오래된 기술이라 화질 저하, 미디어 서버 운영에 대한 높은 난이도 등으로 도태되고 있는 실정.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;애플은 HLS가 개발되기 이전에 &lt;strong&gt;RTSP&lt;/strong&gt; 기반의 QTSS로 비디오와 오디오를 처리했었음
    &lt;ul&gt;
      &lt;li&gt;하지만 QTSS가 애플 생태계에서만 최적화가 되어있었기 때문에 호환성 이유가 있었음&lt;/li&gt;
      &lt;li&gt;RTSP는 HTTP 기반이 아닌 TCP, UDP 사용 
  → 네트워크 상태나 방화벽 문제로 인해 HTTP보다 호환성 낮음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CCTV에서 사용하고 있는 프로토콜&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;스트리머 → rtmp → 서버 → hls → 클라이언트&lt;/p&gt;

&lt;h1 id=&quot;2️⃣hls-hypertext-live-streaming&quot;&gt;2️⃣ HLS (Hypertext Live Streaming)&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고자료&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[https://medium.com/@cwh./%EB%8F%99%EC%98%81%EC%83%81-%EC%95%88%EB%81%8A%EA%B8%B0%EA%B3%A0-%EB%B3%B4%EB%8A%94%EB%B2%95-feat-ll-hls-a6a0c453b21f](https://medium.com/@cwh./%EB%8F%99%EC%98%81%EC%83%81-%EC%95%88%EB%81%8A%EA%B8%B0%EA%B3%A0-%EB%B3%B4%EB%8A%94%EB%B2%95-feat-ll-hls-a6a0c453b21f)


[https://medium.com/@delivalue100/hls-http-live-streaming-4fb1d2992d57](https://medium.com/@delivalue100/hls-http-live-streaming-4fb1d2992d57)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;왜-만들어졌을까-feat-rtsp&quot;&gt;왜 만들어졌을까? feat. RTSP&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;RTSP는 고정된 비트레이트로 콘텐츠 스트리밍에 최적화
    &lt;ul&gt;
      &lt;li&gt;고정 비트레이트의 경우 한가지 속도로만 데이터를 전송하다 보니 네트워크 상태가 변할 때에도 동일한 비트레이트로 스트리밍을 유지해야한다는 문제가 있음&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;⇒ 따라서 호환성이 높은 HTTP를 기반으로, &lt;strong&gt;네트워크 상태에 따라 비디오 품질을 실시간으로 조정&lt;/strong&gt; 가능한 &lt;strong&gt;가변 비트레이트&lt;/strong&gt; 기능까지 도입된 것이 &lt;strong&gt;HLS&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;동작-원리-이해하기&quot;&gt;동작 원리 이해하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;HTTP기반의 단방향 미디어 스트리밍 프로토콜&lt;/li&gt;
  &lt;li&gt;일반 웹서버 + CDN을 활용해 콘텐츠 배포&lt;/li&gt;
  &lt;li&gt;[비디오 → HTTP 파일 조각으로 나눔 → 전송 →] 재생&lt;/li&gt;
  &lt;li&gt;HTTP 파일로 나누기 때문에 별도의 전용 서버가 필요하지 않다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/0.png&quot; alt=&quot;0&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/1.png&quot; alt=&quot;1&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;m3u8 : 메타데이터
    &lt;ul&gt;
      &lt;li&gt;시작 태그&lt;/li&gt;
      &lt;li&gt;hls 프로토콜 버전&lt;/li&gt;
      &lt;li&gt;세그먼트의 최대 길이&lt;/li&gt;
      &lt;li&gt;첫 번째 세그먼트의 시퀀스 번호&lt;/li&gt;
      &lt;li&gt;실제 세그먼트의 길이, 다음 세그먼트 인덱스&lt;/li&gt;
      &lt;li&gt;종료 태그&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ts : 실제 비디오의 조각들을 담고 있는 컨테이너&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;왜-다른-프로토콜에-비해-지연시간이-길까&quot;&gt;왜 다른 프로토콜에 비해 지연시간이 길까?&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/2.png&quot; alt=&quot;2&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;지연시간이 긴 것에는 다양한 원인이 있었음&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;UDP 를 사용하는 다른 프로토콜들과 달리 HLS 는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TCP&lt;/code&gt; 를 사용한다.&lt;/li&gt;
  &lt;li&gt;HLS 는 오히려 실시간에 집중하기 보다는 데이터의 신뢰성과 효율성에 집중한 프로토콜이다.&lt;/li&gt;
  &lt;li&gt;동작 방식으로 인한 지연
    &lt;ol&gt;
      &lt;li&gt;서버에서 스트리밍할 비디오 파일을 짧은 세그먼트(2~10초)로 분할 
 → 재생을 시작하기 위해선 &lt;strong&gt;적어도 몇 개의 세그먼트가 필요해 초기 버퍼링 시간이 길어짐&lt;/strong&gt; (세그먼트 길이가 길수록 지연이 커질 수 있음)
 ⇒ CDN으로 세그먼트 캐싱해서 데이터 전송 시간을 개선하려함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;HTTP 기반 요청-응답 지연
    &lt;ol&gt;
      &lt;li&gt;HTTP 기반이므로 &lt;strong&gt;각 세그먼트 마다 서버에 요청을 보내고 응답을 기다리는 시간&lt;/strong&gt;이 필요. 네트워크의 상태나 응답 속도에 종속&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;가변 비트레이트 지연
    &lt;ol&gt;
      &lt;li&gt;네트워크 상태에 따라 &lt;strong&gt;최적의 비트레이트를 선택하기 위한 모니터링&lt;/strong&gt; 진행 
 → 이 과정에서 비트레이트를 전환하며 지연이 발생함 
 ⇒ 지연이 시청 환경을 개선하지만 실시간성에서는 불리함!&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;CDN 캐싱과 서버 최적화 이슈
    &lt;ol&gt;
      &lt;li&gt;세그먼트 파일이 여러 서버에 캐시되어도 여전히 HTTP 기반으로 동작해 추가적인 통신과정 필요&lt;/li&gt;
      &lt;li&gt;특히 사용자 많은 라이브방송에서는 CDN이 적절히 분배되지 않으면 지연 발생&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;⇒ 따라서 HLS는 라이브 스트리밍보다는 정적 비디오 플레이어에 더 적합&lt;/p&gt;

&lt;h3 id=&quot;치지직에서의-hls-사용법&quot;&gt;치지직에서의 HLS 사용법&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/3.png&quot; alt=&quot;3&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;⇒ 결국 클라이언트가 플레이리스트의 미디어 세그먼트들을 순차적으로 GET하고 마지막쯤에 새로운 세그먼트를 요청하는 흐름이기 때문에 &lt;strong&gt;단방향 통신으로 동작한다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;+) 뒤로 돌려보기 안되는 이유..? 돌려보기 자체에 리소스 소모가 큰듯. 그냥 방송 종료 후 다시보기 기능을 넣는게 이득&lt;/p&gt;

&lt;h3 id=&quot;지연시간-어떻게-개선할-수-있을까&quot;&gt;지연시간 어떻게 개선할 수 있을까?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cdnetworks.com/ko/blog/media-delivery/http-live-streaming/&quot;&gt;https://www.cdnetworks.com/ko/blog/media-delivery/http-live-streaming/&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;세그먼트 길이 단축
    &lt;ol&gt;
      &lt;li&gt;6초 이상의 기본적인 세그먼트 길이를 2~1초로 줄이기
        &lt;ol&gt;
          &lt;li&gt;짧아질수록 서버와 클라이언트 사이에 요청이 많아 부하가 생길 수 있으니 균형점을 찾아야함&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;LL-HLS 도입
    &lt;ol&gt;
      &lt;li&gt;애플이 개발한 HLS의 초저지연 버전&lt;/li&gt;
      &lt;li&gt;세그먼트 내에서 더 작은 단위인 파트로 나눠 전송하며, 세그먼트가 완전히 준비되지 않아도 일부를 먼저 전송함&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;세그먼트 프리페칭
    &lt;ol&gt;
      &lt;li&gt;클라이언트가 다음 세그먼트를 미리 예측하고 사전 다운로드&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;송출 환경 최적화
    &lt;ol&gt;
      &lt;li&gt;치지직 공지사항을 보니 송출 프로그램을 최적화하는 방법도..&lt;/li&gt;
      &lt;li&gt;스트리머가 키프레임 간격을 너무 짧게 설정 →세그먼트가 너무 짧아짐, 서버 부하 ⇒ 버퍼링 많아짐 이슈인듯
        &lt;ol&gt;
          &lt;li&gt;&lt;a href=&quot;https://help.naver.com/notice/noticeView.help?noticeNo=17188&amp;amp;serviceNo=23946&amp;amp;lang=ko&quot;&gt;https://help.naver.com/notice/noticeView.help?noticeNo=17188&amp;amp;serviceNo=23946&amp;amp;lang=ko&lt;/a&gt;&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;비트레이트와-세그먼트&quot;&gt;비트레이트와 세그먼트&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;비트레이트&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m3u8&lt;/code&gt; 플레이리스트 파일 수정
        &lt;ol&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXT-X-STREAM-INF&lt;/code&gt; 태그를 사용하여 비트레이트를 설정&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
			#EXTM3U
			#EXT-X-VERSION:3
			
			#EXT-X-STREAM-INF:BANDWIDTH=1500000
			# 이 스트림에 대한 메타데이터를 정의
			# BANDWIDTH=1500000은 이 스트림의 대역폭 요구 사항이 1,500,000 비트(1.5 Mbps)임을 나타냄
			
			stream_1500.m3u8
			
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	2. 추가적인 태그 설명
		- **#EXT-X-DISCONTINUITY**: 이전 세그먼트와의 **연속성이 끊어짐**을 표시합니다. 이는 광고와 같은 **다른 유형의 미디어를 삽입**할 때 사용됩니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
				m3u8
				#EXT-X-DISCONTINUITY
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;			- **예시**: 이 태그 뒤에 오는 세그먼트는 이전 미디어와는 다르게 인코딩되었거나 다른 타입의 미디어임을 의미합니다.
		- **#EXT-X-STREAM-INF**: **다중 비트레이트 스트림** 또는 **적응형 스트리밍**을 위해 사용됩니다. 여러 비트레이트를 가진 대체 스트림이 있을 때, 클라이언트가 네트워크 상태에 따라 적절한 스트림을 선택할 수 있도록 합니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
				m3u8
				#EXT-X-STREAM-INF:BANDWIDTH=800000,RESOLUTION=640x360
				low.m3u8
				#EXT-X-STREAM-INF:BANDWIDTH=1500000,RESOLUTION=1280x720
				medium.m3u8
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;			- **예시**: 800kbps, 1500kbps로 각각 다른 해상도의 스트림을 제공합니다. 클라이언트는 네트워크 상황에 따라 적절한 `m3u8` 파일을 선택하게 됩니다.
		- **#EXT-X-KEY**: 세그먼트의 **암호화 키 정보**를 제공합니다. HLS에서는 콘텐츠 보호를 위해 **AES-128 암호화**를 사용하여 세그먼트를 암호화할 수 있습니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
				m3u8
				#EXT-X-KEY:METHOD=AES-128,URI=&quot;https://example.com/key&quot;
				
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;			- **예시**: 세그먼트를 AES-128로 암호화하며, 암호화 키를 가져올 위치는 `https://example.com/key`입니다. 2. **세그먼트**
1. FFmpeg로 비디오를 여러 해상도로 인코딩할 때 `-hls_time`을 사용하여 세그먼트의 길이를 설정 (기본 10초)
2. [https://frontdev.tistory.com/entry/ffmpeg%EB%A1%9C-hls-%EB%A7%8C%EB%93%A4%EA%B8%B0-%EC%98%B5%EC%85%98%EC%A0%95%EB%A6%AC](https://frontdev.tistory.com/entry/ffmpeg%EB%A1%9C-hls-%EB%A7%8C%EB%93%A4%EA%B8%B0-%EC%98%B5%EC%85%98%EC%A0%95%EB%A6%AC)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
		- ffmpeg -i hasashin.mp4 -b:v 1M -g 60 -hls_time 2 -hls_list_size 0 -hls_segment_size 500000 output.m3u8
		출처: https://frontdev.tistory.com/entry/ffmpeg로-hls-만들기-옵션정리 [Front End Develop:티스토리]
		
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;결론
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;높은 비트레이트와 긴 세그먼트&lt;/strong&gt;: 높은 비트레이트를 사용하면서 긴 세그먼트를 전송할 경우, 네트워크 대역폭을 효율적으로 사용할 수 있지만, 지연 시간이 증가. 특히 불안정한 네트워크에서는 재생 품질이 저하됨.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;낮은 비트레이트와 짧은 세그먼트&lt;/strong&gt;: 낮은 비트레이트와 짧은 세그먼트를 조합하면, 재생 안정성이 높아지고 지연 시간 감소, 비디오 품질은 저하. ⇒ 이게 우리서비스에 적합하지 않을까?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ll-hls의-지연시간-개선-방법&quot;&gt;LL-HLS의 지연시간 개선 방법&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/4.png&quot; alt=&quot;4&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;하나당 10초 정도인 ts 가 아닌 1초 이하 정도의 CMAF 컨테이너에 담아서 생성 즉시 전송한다.&lt;/li&gt;
  &lt;li&gt;또한 m3u8 파일에 추가적인 메타데이터가 들어간다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ll-hls-관련-메타데이터&quot;&gt;LL-HLS 관련 메타데이터&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;#EXT-X-PART-INF&lt;/strong&gt;: &lt;strong&gt;저지연 HLS&lt;/strong&gt;에서 사용되는 태그로, 세그먼트가 &lt;strong&gt;파편(chunk)&lt;/strong&gt;으로 나뉘어 전송되는 경우 각 파편의 정보를 포함합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
	m3u8
	#EXT-X-PART-INF:PART-TARGET=1.0
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- **예시**: 각 파편의 타겟 길이가 **1초**임을 의미합니다. - **#EXT-X-PRELOAD-HINT**: LL-HLS에서 아직 완료되지 않은 세그먼트나 파편에 대해 **미리 가져올 힌트**를 제공합니다. 이를 통해 클라이언트가 지연 시간을 줄이기 위해 미리 준비할 수 있습니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;text
	m3u8
	#EXT-X-PRELOAD-HINT:TYPE=PART,URI=&quot;segment3_part1.ts&quot;
	
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- **예시**: `segment3_part1.ts` 파편을 미리 로드할 힌트입니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;응답-지연&quot;&gt;응답 지연&lt;/h2&gt;

&lt;p&gt;저번 금요일에 회고 시간에,  Short Polling, Long Polling 이 나왔던 적이 있었는데 이와 비슷한 방식으로 지연 시간을 줄였다고 보면 쉬울 것 같다.&lt;/p&gt;

&lt;p&gt;Short Polling 은 주기적으로 서버에 요청을 보내서 업데이트 된 사항이 있는지를 체크한다. 만약 업데이트된 사항이 없다면 서버는 304(Not Modified), 200 를 반환한다.&lt;/p&gt;

&lt;p&gt;이때 클라이언트는 업데이트된 사항이 있을 때 까지 또 요청을 보낸다.&lt;/p&gt;

&lt;p&gt;Long Polling (200) 은 클라이언트가 서버에 요청을 보냈을 때, 업데이트가 되기 전까지는 서버가 응답을 되돌려주지 않고 연결을 유지(지연)하다가, 업데이트가 된 순간 응답을 보낸다.&lt;/p&gt;

&lt;p&gt;기존 HLS는 메타데이터(플레이리스트) 인 .m3u8 파일을 지속적으로 서버에 요청해서 .m3u8 을 토대로 세그먼트를 재생하는 방식인데, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;지속적으로 서버에 요청&lt;/code&gt; 이 과정이 지연시간의 주범이다. 이 시간 동안 클라이언트는 세그먼트를 받는게 아니라 대기를 해버리기 때문에, 실제 동영상 파일인 세그먼트를 업데이트가 되고 나서야 받을 수 있다.&lt;/p&gt;

&lt;p&gt;기존 HLS 는 HTTP 요청을 주기적으로 보내서 .m3u8 이 업데이트가 되었는지 확인을 한다. 만약 업데이트가 되지 않았다면 또 서버에 요청을 보내고, 업데이트가 되었다는 응답을 받고 나서야 실제 스트리밍 데이터인 세그먼트를 받아온다.&lt;/p&gt;

&lt;p&gt;LL-HLS 는&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;클라이언트의 플레이리스트 요청&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;클라이언트가 서버에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.m3u8&lt;/code&gt; &lt;strong&gt;플레이리스트&lt;/strong&gt;를 요청합니다. 이때 클라이언트는 &lt;strong&gt;최신 세그먼트&lt;/strong&gt;를 가져오기를 원합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;서버의 요청 지연(Blocking)&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;만약 서버에 &lt;strong&gt;새로운 세그먼트&lt;/strong&gt;가 아직 생성되지 않은 경우, 서버는 즉시 응답을 하지 않고 요청을 &lt;strong&gt;일정 시간 동안 대기(Blocking)&lt;/strong&gt; 시킵니다. 이 대기 시간 동안 서버는 새로운 세그먼트가 생성되기를 기다립니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;새로운 세그먼트 생성 시 응답&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;새로운 세그먼트가 생성되면 서버는 대기 중인 클라이언트의 요청에 응답하여 &lt;strong&gt;최신 플레이리스트&lt;/strong&gt;를 반환합니다. 클라이언트는 이를 통해 바로 다음 세그먼트를 가져가 재생을 시작할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;업데이트 주기 단축&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;이러한 방식은 기존의 주기적인 폴링 방식보다 &lt;strong&gt;지연 시간&lt;/strong&gt;을 훨씬 줄일 수 있습니다. 즉, 클라이언트가 계속해서 플레이리스트를 요청하여 최신 정보를 받기 위해 대기하지 않도록 하여 지연 시간을 줄입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;서버의-부하&quot;&gt;서버의 부하&lt;/h2&gt;

&lt;p&gt;LL-HLS 는 세그먼트를 더 짧게 생성하기 때문에 당연히 세그먼트를 더 자주 생성하게 되고, m3u8 의 업데이트도 더 많이 일어난다. 따라서 서버의 부담이 증가하게 된다.&lt;/p&gt;

&lt;p&gt;서버의 부담을 줄이는 방법은 있을까?&lt;/p&gt;

&lt;h3 id=&quot;서버-부하를-줄이기-위한-방안&quot;&gt;서버 부하를 줄이기 위한 방안&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;CDN(Content Delivery Network) 사용&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;LL-HLS&lt;/strong&gt;에서 서버 부하를 줄이기 위해 가장 많이 사용되는 방법 중 하나는 &lt;strong&gt;CDN&lt;/strong&gt;을 사용하는 것입니다. CDN은 스트리밍 콘텐츠를 여러 지점에 &lt;strong&gt;캐시&lt;/strong&gt;하고, 지리적으로 가까운 사용자에게 콘텐츠를 제공함으로써 &lt;strong&gt;서버의 부하를 분산&lt;/strong&gt;시킵니다.&lt;/li&gt;
      &lt;li&gt;CDN을 사용하면 클라이언트가 직접 서버에 연결하는 대신 CDN에서 콘텐츠를 받아가므로, &lt;strong&gt;서버의 직접적인 요청 수&lt;/strong&gt;를 줄일 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;효율적인 세그먼트 생성 및 캐싱&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;서버는 세그먼트를 효율적으로 생성하고, &lt;strong&gt;재사용 가능한 세그먼트를 캐싱&lt;/strong&gt;함으로써 부하를 줄일 수 있습니다. 특히, 플레이리스트와 세그먼트가 자주 변경되기 때문에, 이를 적절히 캐싱하여 동일한 콘텐츠를 여러 클라이언트가 요청하는 경우 서버에서 재생성할 필요가 없도록 해야 합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;최적의 플레이리스트 및 세그먼트 길이 조정&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;LL-HLS에서는 지연 시간을 줄이기 위해 세그먼트를 작은 크기로 나누지만, &lt;strong&gt;너무 작은 단위의 세그먼트&lt;/strong&gt;는 서버 부하를 크게 증가시킬 수 있습니다. 따라서 &lt;strong&gt;적절한 세그먼트 길이&lt;/strong&gt;와 &lt;strong&gt;플레이리스트 갱신 주기&lt;/strong&gt;를 설정하여 서버 부하와 지연 시간을 균형 있게 맞추는 것이 중요합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;대신 safari 를 제외한 브라우저에서는 &lt;video&gt; 태그만으로는 재생이 불가능하다. HLS 스트림 변환이 필요하다.&lt;/video&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hls.js → hls 스트림을 브라우저가 이해할 수 있는 포맷으로 변환하여 &lt;video&gt; 요소에 전달하는 라이브러리&lt;/video&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3️⃣webrtc-web-real-time-communication&quot;&gt;3️⃣ WebRTC (Web Real-Time Communication)&lt;/h1&gt;

&lt;p&gt;웹 브라우저 간에 플러그인의 도움 없이 서로 통신할 수 있도록 설계된 &lt;strong&gt;Javascript API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;→ 별다른 소프트웨어 없이 카메라, 마이크 등을 사용하여 실시간 커뮤니케이션을 제공&lt;/p&gt;

&lt;p&gt;음성 통화, 영상 통화, P2P 파일 공유 등으로 활용됨&lt;/p&gt;

&lt;h2 id=&quot;webrtc의-장점&quot;&gt;WebRTC의 장점&lt;/h2&gt;

&lt;h3 id=&quot;webrtc는-낮은-latency를-갖는다&quot;&gt;WebRTC는 낮은 Latency를 갖는다&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;WebRTC는 &lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P2P(peer-to-peer)방식&lt;/code&gt;&lt;/strong&gt;으로 데이터를 전송
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;P2P&lt;/strong&gt;는 &lt;strong&gt;중간 서버&lt;/strong&gt;를 거치지 않음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;WebRTC는 &lt;strong&gt;UDP 기반&lt;/strong&gt;으로 작동하여 신속하게 패킷을 전송
    &lt;ul&gt;
      &lt;li&gt;HLS와 RTMP는 &lt;strong&gt;TCP 기반&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;webrtc는-호환성이-높다&quot;&gt;WebRTC는 호환성이 높다&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/5.png&quot; alt=&quot;5&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;webrtc의-단점&quot;&gt;WebRTC의 단점&lt;/h2&gt;

&lt;h3 id=&quot;많은-사용자가-사용할-수-없다-스케일링-문제&quot;&gt;많은 사용자가 사용할 수 없다 (스케일링 문제)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;WebRTC는 P2P(peer-to-peer) 구조로 작동하기 때문에 각 사용자 간의 직접적인 연결을 설정&lt;/li&gt;
  &lt;li&gt;사용자가 많아질수록 필요한 연결 수가 &lt;strong&gt;기하급수적으로 증가&lt;/strong&gt;함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/6.png&quot; alt=&quot;6&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;N명의 사용자가 있을 때 사용자 한명이 추가되면 N개의 연결이 필요
    &lt;ul&gt;
      &lt;li&gt;서버와 클라이언트 모두에 큰 부담&lt;/li&gt;
      &lt;li&gt;네트워크 대역폭과 성능 저하 초래&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;화질-문제-성능문제&quot;&gt;화질 문제 (성능문제)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;실시간 비디오와 오디오 스트리밍을 위해 상당한 대역폭을 소모&lt;/li&gt;
  &lt;li&gt;특히 고화질 비디오 스트림은 많은 대역폭을 요구&lt;/li&gt;
  &lt;li&gt;대역폭이 제한된 환경에서는 &lt;strong&gt;패킷 손실&lt;/strong&gt;이 발생할 수 있으며, 이는 전체 스트림의 품질 저하로 이어짐
    &lt;ul&gt;
      &lt;li&gt;WebRTC는 &lt;strong&gt;UDP 기반&lt;/strong&gt;이므로 패킷 손실이 일어날 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다수의 사용자가 동시에 스트리밍을 시도하면 &lt;strong&gt;대역폭이 고갈&lt;/strong&gt;될 위험&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;대규모-라이브-방송에-불완전함&quot;&gt;대규모 라이브 방송에 불완전함&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;예측 불가능한 대역폭 소모&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;HLS나 RTMP&lt;/strong&gt;의 예측 가능한 방식
        &lt;ul&gt;
          &lt;li&gt;HLS나 RTMP는 중앙 서버에서 스트리밍을 관리하고 일반적으로 미리 인코딩된 비디오 조각을 전송&lt;/li&gt;
          &lt;li&gt;스케일링 가능 : 수많은 사용자에게 콘텐츠를 효율적으로 배포할 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;WebRTC&lt;/strong&gt;는 미리 인코딩된 스트림을 사용하지 않기 때문에 대역폭이 비디오 품질과 사용자 수에 따라 즉각적으로 변동함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;webrtc보다-빠른-기술이-있을까&quot;&gt;WebRTC보다 빠른 기술이 있을까?&lt;/h2&gt;

&lt;h3 id=&quot;webtransport&quot;&gt;&lt;strong&gt;WebTransport&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;WebTransport는 UDP 기반으로 통신&lt;/li&gt;
  &lt;li&gt;WebRTC의 signaling 없이도 서버와의 실시간 데이터 교환 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 WebTransport는 아직 개발 단계이며, 안정적인 표준화가 이루어지지 않아 모든 브라우저에서의 지원이 불확실&lt;/p&gt;

&lt;p&gt;WebRTC처럼 실시간 오디오와 비디오 전송을 위한 최적화가 충분하지 않음&lt;/p&gt;

&lt;h3 id=&quot;hls-vs-webrtc-비교&quot;&gt;HLS vs WebRTC 비교&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**HLS**(HTTP Live Streaming)와 **WebRTC**는 모두 실시간 스트리밍을 위한 기술이지만, **사용 목적**, **지연 시간**, **보안** 및 **호환성** 측면에서 큰 차이가 있습니다. 사용하려는 애플리케이션의 유형에 따라 각 기술의 장단점을 비교해보고 어떤 것이 더 적합한지 선택해야 합니다. 아래는 HLS와 WebRTC를 실시간 스트리밍 웹 서비스의 관점에서 비교한 것입니다.


### 1. 지연 시간

- **HLS**:
	- **지연 시간**: HLS는 HTTP 기반 프로토콜로 설계되어 있으며, **10~30초**의 지연 시간이 발생하는 것이 일반적입니다. 심지어 **저지연 HLS(LL-HLS)**를 사용하더라도 2~5초 정도의 지연 시간이 존재할 수 있습니다.
	- **용도**: 따라서 HLS는 실시간성이 요구되지 않는 스트리밍에 적합합니다. 예를 들어, 뉴스 방송, 스포츠 경기, 강의 등의 상황에서 몇 초 정도의 지연이 큰 문제가 되지 않는다면 HLS가 적합합니다.
- **WebRTC**:
	- **지연 시간**: WebRTC는 **1초 미만의 매우 낮은 지연 시간**을 목표로 설계되어, 실시간 인터랙티브한 통신에 적합합니다.
	- **용도**: 화상 회의, 게임 스트리밍, 실시간 원격 조작 등 **매우 빠른 반응**이 필요한 애플리케이션에 적합합니다. WebRTC의 P2P 연결 방식 덕분에 지연 시간을 최소화할 수 있습니다.

### 2. 사용 사례

- **HLS**:
	- **주로 대규모 시청자 대상의 스트리밍**: HLS는 대규모 스트리밍 서비스(예: YouTube, Twitch 등)에서 매우 효과적입니다. HTTP 기반이라 기존 **CDN(Content Delivery Network)** 인프라를 활용해 손쉽게 콘텐츠를 전송할 수 있으며, 수천에서 수백만 명의 시청자가 동시에 스트리밍을 보는 데 적합합니다.
	- **적응형 스트리밍**: 네트워크 상태에 따라 비디오 품질을 조정하는 **적응형 스트리밍**을 기본적으로 제공하므로, 시청자의 네트워크 환경에 맞춰 끊김 없이 서비스를 제공합니다.
- **WebRTC**:
	- **1:1 혹은 소규모 그룹 통신**: WebRTC는 **화상 통화**, **화상 회의**, **온라인 협업 툴** 등 소규모 실시간 커뮤니케이션에 최적화되어 있습니다. 또한, 게임 스트리밍과 같은 실시간 반응이 중요한 상황에도 적합합니다.
	- **P2P 연결**: WebRTC는 주로 **P2P 연결**을 통해 데이터를 직접 전송하므로 서버의 부하를 줄일 수 있지만, 대규모 시청자를 대상으로 하기에 효율적이지 않을 수 있습니다.

### 3. 확장성

- **HLS**:
	- **대규모 스트리밍에 최적**: HLS는 HTTP 기반이기 때문에 **CDN**을 사용하여 확장성을 쉽게 확보할 수 있습니다. 수천 명 이상의 사용자가 동시에 스트리밍을 소비할 수 있으며, 서버 부담을 덜 수 있습니다.
- **WebRTC**:
	- **제한된 확장성**: WebRTC는 브라우저 간 **P2P 연결**을 사용하므로, 직접 연결의 수가 증가할수록 확장성이 제한됩니다. 예를 들어, 많은 참가자 간의 연결이 필요하면 각 클라이언트의 네트워크 및 CPU 자원이 급격히 소모됩니다.
	- **SFU 사용**: 확장성을 늘리기 위해 **SFU(Selective Forwarding Unit)**를 도입하여 각 클라이언트가 모든 참가자와 직접 연결하지 않고 중앙 서버를 통해 연결을 관리할 수 있도록 할 수 있습니다. 이를 통해 약 100명 이상의 사용자까지 확장 가능합니다.

### 4. 네트워크 호환성 및 안정성

- **HLS**:
	- **HTTP 기반 전송**: HLS는 HTTP 기반이기 때문에, 대부분의 방화벽을 통과하고 네트워크 호환성이 매우 뛰어납니다.
	- **안정적**: HTTP와 TCP를 사용하여 데이터 전송이 신뢰적이며, 중간에 발생하는 패킷 손실을 재전송하는 메커니즘이 있어 안정적인 스트리밍을 제공합니다.
- **WebRTC**:
	- **NAT Traversal**: WebRTC는 P2P 연결을 위해 **STUN** 및 **TURN** 서버를 사용하여 NAT 뒤에 있는 클라이언트를 연결합니다. 하지만 네트워크 환경에 따라 연결 설정이 복잡해지거나 문제가 발생할 수 있습니다.
	- **UDP 기반 전송**: 주로 **UDP**를 사용하여 낮은 지연 시간을 제공하지만, 패킷 손실 시 재전송을 보장하지 않아 네트워크 상태가 좋지 않을 때 품질 저하가 발생할 수 있습니다.

### 5. 보안

- **HLS**:
	- **HTTPS와 함께 사용**: HLS는 HTTP 기반으로, **HTTPS**를 사용해 데이터를 암호화할 수 있습니다. 또한, **DRM(디지털 권리 관리)**과 함께 사용해 콘텐츠 보호를 구현할 수 있습니다.
- **WebRTC**:
	- **기본적으로 암호화된 통신**: WebRTC는 모든 오디오, 비디오, 데이터 스트림을 **DTLS**(Datagram Transport Layer Security)와 **SRTP**(Secure Real-Time Transport Protocol)를 사용해 암호화합니다. 기본적으로 강력한 보안이 내장되어 있습니다.

### 6. 브라우저 지원

- **HLS**:
	- **Safari와 iOS 네이티브 지원**: Apple 기기와 Safari 브라우저에서 기본적으로 지원하지만, Chrome, Firefox 등 다른 브라우저에서는 JavaScript 라이브러리(**hls.js**)가 필요합니다.
- **WebRTC**:
	- **모든 최신 브라우저 지원**: WebRTC는 **Chrome**, **Firefox**, **Safari**, **Edge** 등 대부분의 최신 브라우저에서 네이티브로 지원됩니다. 추가적인 플러그인이 필요 없이 실시간 통신 기능을 사용할 수 있습니다.

### 결론

- **HLS**는 **대규모 스트리밍**에 적합하며, 상대적으로 긴 지연 시간을 허용할 수 있는 **방송, 교육, 엔터테인먼트**와 같은 서비스에서 주로 사용됩니다. **HTTP 기반**이므로 네트워크 호환성이 높고, 기존 CDN 인프라를 활용할 수 있는 장점이 있습니다.
- **WebRTC**는 **즉각적인 반응이 필요한 실시간 인터랙티브 애플리케이션**에 적합합니다. **낮은 지연 시간**과 **보안성**을 갖추고 있어 **화상 회의, 실시간 통신, 온라인 협업 도구** 등에 적합하며, 브라우저에서 네이티브로 지원되는 장점이 있습니다. 다만, **확장성** 측면에서는 SFU와 같은 구조적 보완이 필요합니다.

따라서, **대규모 시청자와의 방송**에는 HLS가 적합하고, **소규모 실시간 상호작용**이나 **낮은 지연 시간이 필요한 서비스**에는 WebRTC가 더 나은 선택이 될 것입니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;3️⃣rtmp-real-time-messaging-protocol&quot;&gt;3️⃣ RTMP (Real-Time Messaging Protocol)&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고자료&lt;br /&gt;
&lt;a href=&quot;https://www.cdnetworks.com/ko/blog/media-delivery/what-is-rtmp-ingest/&quot;&gt;https://www.cdnetworks.com/ko/blog/media-delivery/what-is-rtmp-ingest/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://ossrs.net/lts/en-us/docs/v6/doc/flv&quot;&gt;https://ossrs.net/lts/en-us/docs/v6/doc/flv&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://growthvalue.tistory.com/178&quot;&gt;https://growthvalue.tistory.com/178&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://ossrs.net/lts/en-us/docs/v4/doc/delivery-http-flv&quot;&gt;https://ossrs.net/lts/en-us/docs/v4/doc/delivery-http-flv&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://medium.com/@delivalue100/rtmp-realtime-messaging-protocol-c4474e464ffd&quot;&gt;https://medium.com/@delivalue100/rtmp-realtime-messaging-protocol-c4474e464ffd&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://obsproject.com/forum/whats-new/posts/2763754/&quot;&gt;https://obsproject.com/forum/whats-new/posts/2763754/&lt;/a&gt; - OBS 포럼 &lt;br /&gt;
&lt;a href=&quot;https://devocean.sk.com/blog/techBoardDetail.do?ID=164296&quot;&gt;https://devocean.sk.com/blog/techBoardDetail.do?ID=164296&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://blog.naver.com/mingyo01/222050438291&quot;&gt;https://blog.naver.com/mingyo01/222050438291&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://velog.io/@chosj1526/%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8A%B8%EC%9C%84%EC%B9%98-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8&quot;&gt;https://velog.io/@chosj1526/%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8A%B8%EC%9C%84%EC%B9%98-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;개요&quot;&gt;&lt;strong&gt;개요&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/7.png&quot; alt=&quot;7&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/8.png&quot; alt=&quot;8&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;개발자:&lt;/strong&gt; Adobe Systems (원래 Macromedia가 개발)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;출시 시기:&lt;/strong&gt; 2003년&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;주요 용도:&lt;/strong&gt; 실시간 비디오 및 오디오 스트리밍, 특히 라이브 방송&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;특징:&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;어도브에서 규정한 만큼 기존에는 Flash Player을 지원하기 위해 작성된 기술이었다.&lt;/p&gt;

    &lt;p&gt;그러나 최근 어도브에서 Flash Player의 지원을 중단한 만큼 점차 사용률이 저조해지고 있다.&lt;/p&gt;

    &lt;p&gt;하지만 이는 클라이언트 단에서의 문제점이고, 영상 데이터를 서버로 옮기고 저장하는 데에 있어서는 높은 지연 시간과 효율을 가지고 있기에, HLS, MPEG-DASH, HTTP-FLV와 같은 기술과 함께 사용된다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;저지연 스트리밍:&lt;/strong&gt; 실시간 스트리밍에 적합한 낮은 지연 시간&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;양방향 통신:&lt;/strong&gt; 클라이언트와 서버 간의 실시간 데이터 전송 가능&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;유연성:&lt;/strong&gt; 비디오, 오디오, 데이터 스트림을 동시에 전송&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;지연시간이-빠른-이유&quot;&gt;지연시간이 빠른 이유&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;지속적인 연결 유지&lt;/strong&gt;: RTMP는 클라이언트와 서버 간에 &lt;strong&gt;지속적인 TCP 연결&lt;/strong&gt;을 유지합니다. 이는 데이터 전송 시마다 새로운 연결을 설정할 필요가 없기 때문에 &lt;strong&gt;연결 설정에 따른 오버헤드&lt;/strong&gt;를 줄여줍니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;작은 청크(chunk) 단위 전송&lt;/strong&gt;: 데이터를 &lt;strong&gt;작은 청크로 분할하여 전송&lt;/strong&gt;함으로써, 데이터가 준비되는 즉시 전송할 수 있습니다. 이는 &lt;strong&gt;버퍼링 시간을 최소화&lt;/strong&gt;하고, 실시간 성능을 향상시킵니다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;트위치의 경우 스트리머 → Ingest → Transcode → Replication → Edge → 시청자를 거치며 스트리밍 데이터를 전송&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/9.png&quot; alt=&quot;9&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ingest - 스트리머의 비디오 영상이 트위치 데이터 센터로 가는 것&lt;/li&gt;
  &lt;li&gt;Transcode - 비디오 형식을 바꾸는 것&lt;/li&gt;
  &lt;li&gt;Replication - 복사. 안정성을 위해&lt;/li&gt;
  &lt;li&gt;Edge - CDN이라고도 부름&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/10.png&quot; alt=&quot;10&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/11.png&quot; alt=&quot;11&quot; /&gt;&lt;em&gt;image.png&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;한계점&quot;&gt;&lt;strong&gt;한계점&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;보안:&lt;/strong&gt; 기본적으로 보안 기능이 내장되어 있지 않아 데이터 암호화가 필요할 경우 추가 설정이 필요함&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;방화벽 문제:&lt;/strong&gt; 전용 포트(기본적으로 1935)를 사용하므로 일부 네트워크 환경에서는 차단될 수 있음&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;모바일 지원 부족:&lt;/strong&gt; HTTP 기반 스트리밍 프로토콜(HLS, MPEG-DASH)에 비해 모바일 기기에서의 지원이 제한적&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;확장성:&lt;/strong&gt; 대규모&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;4️⃣srt-secure-reliable-transport&quot;&gt;&lt;strong&gt;4️⃣ SRT (Secure Reliable Transport)&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;OBS 사용&lt;/p&gt;

&lt;h2 id=&quot;개요-1&quot;&gt;&lt;strong&gt;개요&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;개발자:&lt;/strong&gt; Haivision&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;출시 시기:&lt;/strong&gt; 2017년&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;주요 용도:&lt;/strong&gt; 불안정한 네트워크 환경에서도 안정적이고 보안이 강화된 비디오 스트리밍&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;특징:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;보안 강화:&lt;/strong&gt; AES 암호화를 통해 데이터 전송 시 보안을 보장&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;신뢰성:&lt;/strong&gt; 패킷 손실, 지연, 네트워크 변동성에 강한 내성을 가짐&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;적응성:&lt;/strong&gt; 다양한 네트워크 조건에 맞춰 동적으로 조정&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;오픈 소스:&lt;/strong&gt; SRT는 오픈 소스 프로젝트로, 다양한 플랫폼과 쉽게 통합 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;srt의-주요-기능&quot;&gt;&lt;strong&gt;SRT의 주요 기능&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;암호화:&lt;/strong&gt; 전송 중인 데이터를 암호화하여 도청 및 데이터 변조를 방지&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;재전송 메커니즘:&lt;/strong&gt; 패킷 손실 시 재전송을 통해 데이터의 완전성을 유지&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;동적 비트레이트 조정:&lt;/strong&gt; 네트워크 상태에 따라 비트레이트를 자동으로 조정하여 스트리밍 품질을 최적화&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;방화벽 우회:&lt;/strong&gt; UDP 기반이지만, NAT 및 방화벽 환경에서도 안정적으로 동작&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;장점&quot;&gt;&lt;strong&gt;장점&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;보안:&lt;/strong&gt; RTMP와 달리 기본적으로 데이터 암호화를 지원하여 보안성이 뛰어남&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;신뢰성:&lt;/strong&gt; 불안정한 네트워크 환경에서도 안정적인 데이터 전송을 보장&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;유연성:&lt;/strong&gt; 다양한 네트워크 조건에 적응하여 최적의 스트리밍 품질 제공&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;확장성:&lt;/strong&gt; 대규모 스트리밍 환경에서도 효율적으로 확장 가능&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;오픈 소스:&lt;/strong&gt; 무료로 사용 가능하며, 커뮤니티 지원을 통해 지속적으로 개선됨&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;단점&quot;&gt;&lt;strong&gt;단점&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;채택률:&lt;/strong&gt; RTMP에 비해 상대적으로 최근에 등장한 프로토콜로, 기존 인프라와의 호환성 문제 발생 가능&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;설정 복잡성:&lt;/strong&gt; 초기 설정과 최적화를 위해 기술적인 지식이 필요할 수 있음&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rtmp와-srt의-비교&quot;&gt;&lt;strong&gt;RTMP와 SRT의 비교&lt;/strong&gt;&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;strong&gt;특징&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;RTMP&lt;/strong&gt;&lt;/th&gt;
      &lt;th&gt;&lt;strong&gt;SRT&lt;/strong&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;보안&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;기본적으로 보안 기능 없음 (RTMPS로 보안 강화 가능)&lt;/td&gt;
      &lt;td&gt;AES 암호화 내장, 기본적으로 보안 강화&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;전송 프로토콜&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;TCP 기반&lt;/td&gt;
      &lt;td&gt;UDP 기반&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;신뢰성&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;TCP의 신뢰성 제공, 그러나 네트워크 변동성에 취약&lt;/td&gt;
      &lt;td&gt;패킷 손실 복구, 네트워크 변동성에 강한 내성&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;지연 시간&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;낮음&lt;/td&gt;
      &lt;td&gt;낮음&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;방화벽 우회&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;전용 포트 사용, 방화벽 문제 발생 가능&lt;/td&gt;
      &lt;td&gt;NAT 및 방화벽 환경에서도 안정적 동작&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;확장성&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;대규모 분산 환경에서 추가 설정 필요&lt;/td&gt;
      &lt;td&gt;대규모 스트리밍 환경에서 효율적으로 확장 가능&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;오픈 소스&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;아니오&lt;/td&gt;
      &lt;td&gt;예 (오픈 소스 프로젝트)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;5️⃣mpeg-dash-dynamic-adaptive-streaming-over-http&quot;&gt;&lt;strong&gt;5️⃣ MPEG-DASH (Dynamic Adaptive Streaming over HTTP)&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;설명:&lt;/strong&gt; ISO 표준의 HTTP 기반 스트리밍 프로토콜로, 적응형 비트레이트를 지원하며 다양한 미디어 형식을 지원합니다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;특징:&lt;/strong&gt; 개방형 표준, 다양한 플랫폼과 호환.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;정리글 원본&lt;/summary&gt;

## 고민지

	- **HLS** **- 지연시간이 높지만 호환성이 좋음 (치지직)**
		- 왜 다른 프로토콜에 비해 지연시간이 길까?
		- 어떻게 지연시간을 낮출 수 있을까?

	### 왜 만들어졌을까? feat. RTSP

	- RTSP는 고정된 비트레이트로 콘텐츠 스트리밍에 최적화
		- 고정 비트레이트의 경우 한가지 속도로만 데이터를 전송하다 보니 네트워크 상태가 변할 때에도 동일한 비트레이트로 스트리밍을 유지해야한다는 문제가 있음

		⇒ 따라서 호환성이 높은 HTTP를 기반으로, **네트워크 상태에 따라 비디오 품질을 실시간으로 조정** 가능한 **가변 비트레이트** 기능까지 도입된 것이 **HLS**


	### HLS(Hypertext Live Streaming)

	- HTTP기반의 단방향 미디어 스트리밍 프로토콜
	- 일반 웹서버 + CDN을 활용해 콘텐츠 배포

	### 왜 다른 프로토콜에 비해 지연시간이 길까?


	![12](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/12.png)_image.png_


	지연시간이 긴 것에는 다양한 원인이 있었음

	1. 동작 방식으로 인한 지연
		1. 서버에서 스트리밍할 비디오 파일을 짧은 세그먼트(2~10초)로 분할 
		→ 재생을 시작하기 위해선 **적어도 몇 개의 세그먼트가 필요해 초기 버퍼링 시간이 길어짐** (세그먼트 길이가 길수록 지연이 커질 수 있음)
		⇒ CDN으로 세그먼트 캐싱해서 데이터 전송 시간을 개선하려함
	2. HTTP 기반 요청-응답 지연
		1. HTTP 기반이므로 **각 세그먼트 마다 서버에 요청을 보내고 응답을 기다리는 시간**이 필요. 네트워크의 상태나 응답 속도에 종속
	3. 가변 비트레이트 지연
		1. 네트워크 상태에 따라 **최적의 비트레이트를 선택하기 위한 모니터링** 진행 
		→ 이 과정에서 비트레이트를 전환하며 지연이 발생함 
		⇒ 지연이 시청 환경을 개선하지만 실시간성에서는 불리함!
	4. CDN 캐싱과 서버 최적화 이슈
		1. 세그먼트 파일이 여러 서버에 캐시되어도 여전히 HTTP 기반으로 동작해 추가적인 통신과정 필요
		2. 특히 사용자 많은 라이브방송에서는 CDN이 적절히 분배되지 않으면 지연 발생

	⇒ 따라서 HLS는 라이브 스트리밍보다는 정적 비디오 플레이어에 더 적합


	### 치지직에서의 HLS 사용법


	[https://medium.com/@delivalue100/hls-http-live-streaming-4fb1d2992d57](https://medium.com/@delivalue100/hls-http-live-streaming-4fb1d2992d57)


	![13](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/13.png)_image.png_


	⇒ 결국 클라이언트가 플레이리스트의 미디어 세그먼트들을 순차적으로 GET하고 마지막쯤에 새로운 세그먼트를 요청하는 흐름이기 때문에 **단방향 통신으로 동작한다.**


	+) 뒤로 돌려보기 안되는 이유..? 돌려보기 자체에 리소스 소모가 큰듯. 그냥 방송 종료 후 다시보기 기능을 넣는게 이득


	### 지연시간 어떻게 개선할 수 있을까?


	[https://www.cdnetworks.com/ko/blog/media-delivery/http-live-streaming/](https://www.cdnetworks.com/ko/blog/media-delivery/http-live-streaming/)

	1. 세그먼트 길이 단축
		1. 6초 이상의 기본적인 세그먼트 길이를 2~1초로 줄이기
			1. 짧아질수록 서버와 클라이언트 사이에 요청이 많아 부하가 생길 수 있으니 균형점을 찾아야함
	2. LL-HLS 도입
		1. 애플이 개발한 HLS의 초저지연 버전
		2. 세그먼트 내에서 더 작은 단위인 파트로 나눠 전송하며, 세그먼트가 완전히 준비되지 않아도 일부를 먼저 전송함
	3. 세그먼트 프리페칭
		1. 클라이언트가 다음 세그먼트를 미리 예측하고 사전 다운로드
	4. 송출 환경 최적화
		1. 치지직 공지사항을 보니 송출 프로그램을 최적화하는 방법도..
		2. 스트리머가 키프레임 간격을 너무 짧게 설정 →세그먼트가 너무 짧아짐, 서버 부하 ⇒ 버퍼링 많아짐 이슈인듯
			1. [https://help.naver.com/notice/noticeView.help?noticeNo=17188&amp;amp;serviceNo=23946&amp;amp;lang=ko](https://help.naver.com/notice/noticeView.help?noticeNo=17188&amp;amp;serviceNo=23946&amp;amp;lang=ko)

	### 치지직의 경우


	기존에 화질이 깨지는 문제 → 서버 부하 이슈로 움직임이 적은 장면에서는 낮은 비트레이트, 빠르게 변하는 장면에서는 높은 비트레이트를 사용해 빠르게 변하는 장면에서 품질이 저하되도록 함.


	⇒ 이후에 서버 장비 도입으로 화질 제한을 풀고 원본 화질에 가깝게 (OBS로 스트리머가 송출하는 화질) 비디오를 전송


	여기서 든 생각 → 어차피 우리의 타겟은 컨퍼런스 발표임, 정적인 장면이 많은 콘텐츠. 초기 치지직 방식대로 낮은 비트레이트를 유지해도 화질 문제는 없을 것 같음. 프레임율 낮추고..


	### 결론

	- **높은 비트레이트와 긴 세그먼트**: 높은 비트레이트를 사용하면서 긴 세그먼트를 전송할 경우, 네트워크 대역폭을 효율적으로 사용할 수 있지만, 지연 시간이 증가. 특히 불안정한 네트워크에서는 재생 품질이 저하됨.
	- **낮은 비트레이트와 짧은 세그먼트**: 낮은 비트레이트와 짧은 세그먼트를 조합하면, 재생 안정성이 높아지고 지연 시간 감소, 비디오 품질은 저하. ⇒ 이게 우리서비스에 적합하지 않을까?

	### 비트레이트랑 세그먼트 조정 어케함?

	1. 비트레이트
		1. `m3u8` 플레이리스트 파일 수정
			1. `EXT-X-STREAM-INF` 태그를 사용하여 비트레이트를 설정

			
```
text
			#EXTM3U
			#EXT-X-VERSION:3
			
			#EXT-X-STREAM-INF:BANDWIDTH=1500000
			# 이 스트림에 대한 메타데이터를 정의
			# BANDWIDTH=1500000은 이 스트림의 대역폭 요구 사항이 1,500,000 비트(1.5 Mbps)임을 나타냄
			
			stream_1500.m3u8
			
```


	2. 세그먼트
		1. FFmpeg로 비디오를 여러 해상도로 인코딩할 때 `-hls_time`을 사용하여 세그먼트의 길이를 설정 (기본 10초)
		2. [https://frontdev.tistory.com/entry/ffmpeg%EB%A1%9C-hls-%EB%A7%8C%EB%93%A4%EA%B8%B0-%EC%98%B5%EC%85%98%EC%A0%95%EB%A6%AC](https://frontdev.tistory.com/entry/ffmpeg%EB%A1%9C-hls-%EB%A7%8C%EB%93%A4%EA%B8%B0-%EC%98%B5%EC%85%98%EC%A0%95%EB%A6%AC)

			
```
text
			- ffmpeg -i hasashin.mp4 -b:v 1M -g 60 -hls_time 2 -hls_list_size 0 -hls_segment_size 500000 output.m3u8
			출처: https://frontdev.tistory.com/entry/ffmpeg로-hls-만들기-옵션정리 [Front End Develop:티스토리]
			
```



## 김영길


	[https://velog.io/@devstefancho/obs-RTMP-%EC%84%9C%EB%B2%84%EB%A1%9C-Live-Streaming](https://velog.io/@devstefancho/obs-RTMP-%EC%84%9C%EB%B2%84%EB%A1%9C-Live-Streaming)


	[https://www.cloudflare.com/ko-kr/learning/video/what-is-http-live-streaming/](https://www.cloudflare.com/ko-kr/learning/video/what-is-http-live-streaming/)


	HLS 는 apple 의 기술이라서 apple 기기에 전부 호환


	반대로 안드로이드나 윈도우에서는 추가적인 로직 필요


	[비디오 → HTTP 파일 조각으로 나눔 → 전송 →] 재생


	HTTP 파일로 나누기 때문에 별도의 전용 서버가 필요하지 않다.


	![14](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/14.png)_image.png_


	![15](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/15.png)_image.png_

	- m3u8 : 메타데이터
		- 시작 태그
		- hls 프로토콜 버전
		- 세그먼트의 최대 길이
		- 첫 번째 세그먼트의 시퀀스 번호
		- 실제 세그먼트의 길이, 다음 세그먼트 인덱스
		- 종료 태그
	- ts : 실제 비디오의 조각들을 담고 있는 컨테이너

	LL-HLS : 저지연


	어떻게 지연 속도를 줄였나


	###  `세그먼트를 또 나누기 -&amp;gt; chunk`


	![16](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/16.png)_image.png_

	- 하나당 10초 정도인 ts 가 아닌 1초 이하 정도의 CMAF 컨테이너에 담아서 생성 즉시 전송한다.

	ㅅ또한 m3u8 파일에 추가적인 메타데이터가 들어간다.


	### 2.1 미디어 변경 또는 광고

	- **#EXT-X-DISCONTINUITY**: 이전 세그먼트와의 **연속성이 끊어짐**을 표시합니다. 이는 광고와 같은 **다른 유형의 미디어를 삽입**할 때 사용됩니다.

		
```
text
		m3u8
		#EXT-X-DISCONTINUITY
		
```


		- **예시**: 이 태그 뒤에 오는 세그먼트는 이전 미디어와는 다르게 인코딩되었거나 다른 타입의 미디어임을 의미합니다.

	### 2.2 변수 비트레이트 및 다중 스트림

	- **#EXT-X-STREAM-INF**: **다중 비트레이트 스트림** 또는 **적응형 스트리밍**을 위해 사용됩니다. 여러 비트레이트를 가진 대체 스트림이 있을 때, 클라이언트가 네트워크 상태에 따라 적절한 스트림을 선택할 수 있도록 합니다.

		
```
text
		m3u8
		#EXT-X-STREAM-INF:BANDWIDTH=800000,RESOLUTION=640x360
		low.m3u8
		#EXT-X-STREAM-INF:BANDWIDTH=1500000,RESOLUTION=1280x720
		medium.m3u8
		
```


		- **예시**: 800kbps, 1500kbps로 각각 다른 해상도의 스트림을 제공합니다. 클라이언트는 네트워크 상황에 따라 적절한 `m3u8` 파일을 선택하게 됩니다.

	### 2.3 키 프레임 및 암호화

	- **#EXT-X-KEY**: 세그먼트의 **암호화 키 정보**를 제공합니다. HLS에서는 콘텐츠 보호를 위해 **AES-128 암호화**를 사용하여 세그먼트를 암호화할 수 있습니다.

		
```
text
		m3u8
		#EXT-X-KEY:METHOD=AES-128,URI=&quot;https://example.com/key&quot;
		
```


		- **예시**: 세그먼트를 AES-128로 암호화하며, 암호화 키를 가져올 위치는 `https://example.com/key`입니다.

	### 2.4 LL-HLS 관련 메타데이터

	- **#EXT-X-PART-INF**: **저지연 HLS**에서 사용되는 태그로, 세그먼트가 **파편(chunk)**으로 나뉘어 전송되는 경우 각 파편의 정보를 포함합니다.

		
```
text
		m3u8
		#EXT-X-PART-INF:PART-TARGET=1.0
		
```


		- **예시**: 각 파편의 타겟 길이가 **1초**임을 의미합니다.
	- **#EXT-X-PRELOAD-HINT**: LL-HLS에서 아직 완료되지 않은 세그먼트나 파편에 대해 **미리 가져올 힌트**를 제공합니다. 이를 통해 클라이언트가 지연 시간을 줄이기 위해 미리 준비할 수 있습니다.

		
```
text
		m3u8
		#EXT-X-PRELOAD-HINT:TYPE=PART,URI=&quot;segment3_part1.ts&quot;
		
```


		- **예시**: `segment3_part1.ts` 파편을 미리 로드할 힌트입니다.

	### `응답 지연`


	저번 금요일에 회고 시간에,  Short Polling, Long Polling 이 나왔던 적이 있었는데 이와 비슷한 방식으로 지연 시간을 줄였다고 보면 쉬울 것 같다.


	Short Polling 은 주기적으로 서버에 요청을 보내서 업데이트 된 사항이 있는지를 체크한다. 만약 업데이트된 사항이 없다면 서버는 304(Not Modified), 200 를 반환한다.


	이때 클라이언트는 업데이트된 사항이 있을 때 까지 또 요청을 보낸다.


	Long Polling (200) 은 클라이언트가 서버에 요청을 보냈을 때, 업데이트가 되기 전까지는 서버가 응답을 되돌려주지 않고 연결을 유지(지연)하다가, 업데이트가 된 순간 응답을 보낸다. 


	기존 HLS는 메타데이터(플레이리스트) 인 .m3u8 파일을 지속적으로 서버에 요청해서 .m3u8 을 토대로 세그먼트를 재생하는 방식인데, `지속적으로 서버에 요청` 이 과정이 지연시간의 주범이다. 이 시간 동안 클라이언트는 세그먼트를 받는게 아니라 대기를 해버리기 때문에, 실제 동영상 파일인 세그먼트를 업데이트가 되고 나서야 받을 수 있다.


	기존 HLS 는 HTTP 요청을 주기적으로 보내서 .m3u8 이 업데이트가 되었는지 확인을 한다. 만약 업데이트가 되지 않았다면 또 서버에 요청을 보내고, 업데이트가 되었다는 응답을 받고 나서야 실제 스트리밍 데이터인 세그먼트를 받아온다. 


	LL-HLS 는 

	- **클라이언트의 플레이리스트 요청**:
		- 클라이언트가 서버에 `.m3u8` **플레이리스트**를 요청합니다. 이때 클라이언트는 **최신 세그먼트**를 가져오기를 원합니다.
	- **서버의 요청 지연(Blocking)**:
		- 만약 서버에 **새로운 세그먼트**가 아직 생성되지 않은 경우, 서버는 즉시 응답을 하지 않고 요청을 **일정 시간 동안 대기(Blocking)** 시킵니다. 이 대기 시간 동안 서버는 새로운 세그먼트가 생성되기를 기다립니다.
	- **새로운 세그먼트 생성 시 응답**:
		- 새로운 세그먼트가 생성되면 서버는 대기 중인 클라이언트의 요청에 응답하여 **최신 플레이리스트**를 반환합니다. 클라이언트는 이를 통해 바로 다음 세그먼트를 가져가 재생을 시작할 수 있습니다.
	- **업데이트 주기 단축**:
		- 이러한 방식은 기존의 주기적인 폴링 방식보다 **지연 시간**을 훨씬 줄일 수 있습니다. 즉, 클라이언트가 계속해서 플레이리스트를 요청하여 최신 정보를 받기 위해 대기하지 않도록 하여 지연 시간을 줄입니다.

	### `서버의 부하`


	LL-HLS 는 세그먼트를 더 짧게 생성하기 때문에 당연히 세그먼트를 더 자주 생성하게 되고, m3u8 의 업데이트도 더 많이 일어난다. 따라서 서버의 부담이 증가하게 된다.


	서버의 부담을 줄이는 방법은 있을까?


	### 2. 서버 부하를 줄이기 위한 방안

	1. **CDN(Content Delivery Network) 사용**
		- **LL-HLS**에서 서버 부하를 줄이기 위해 가장 많이 사용되는 방법 중 하나는 **CDN**을 사용하는 것입니다. CDN은 스트리밍 콘텐츠를 여러 지점에 **캐시**하고, 지리적으로 가까운 사용자에게 콘텐츠를 제공함으로써 **서버의 부하를 분산**시킵니다.
		- CDN을 사용하면 클라이언트가 직접 서버에 연결하는 대신 CDN에서 콘텐츠를 받아가므로, **서버의 직접적인 요청 수**를 줄일 수 있습니다.
	2. **효율적인 세그먼트 생성 및 캐싱**
		- 서버는 세그먼트를 효율적으로 생성하고, **재사용 가능한 세그먼트를 캐싱**함으로써 부하를 줄일 수 있습니다. 특히, 플레이리스트와 세그먼트가 자주 변경되기 때문에, 이를 적절히 캐싱하여 동일한 콘텐츠를 여러 클라이언트가 요청하는 경우 서버에서 재생성할 필요가 없도록 해야 합니다.
	3. **최적의 플레이리스트 및 세그먼트 길이 조정**
		- LL-HLS에서는 지연 시간을 줄이기 위해 세그먼트를 작은 크기로 나누지만, **너무 작은 단위의 세그먼트**는 서버 부하를 크게 증가시킬 수 있습니다. 따라서 **적절한 세그먼트 길이**와 **플레이리스트 갱신 주기**를 설정하여 서버 부하와 지연 시간을 균형 있게 맞추는 것이 중요합니다.

	대신 safari 를 제외한 브라우저에서는 &lt;video&gt; 태그만으로는 재생이 불가능하다. HLS 스트림 변환이 필요하다.

	- hls.js → hls 스트림을 브라우저가 이해할 수 있는 포맷으로 변환하여 &lt;video&gt; 요소에 전달하는 라이브러리
	- 지연시간이 왜 많이 생기는가?
		- UDP 를 사용하는 다른 프로토콜들과 달리 HLS 는 `TCP` 를 사용한다.
		- HLS 는 오히려 실시간에 집중하기 보다는 데이터의 신뢰성과 효율성에 집중한 프로토콜이다.
	- **저지연 HLS(LL-HLS)**
	- [https://medium.com/@cwh./%EB%8F%99%EC%98%81%EC%83%81-%EC%95%88%EB%81%8A%EA%B8%B0%EA%B3%A0-%EB%B3%B4%EB%8A%94%EB%B2%95-feat-ll-hls-a6a0c453b21f](https://medium.com/@cwh./%EB%8F%99%EC%98%81%EC%83%81-%EC%95%88%EB%81%8A%EA%B8%B0%EA%B3%A0-%EB%B3%B4%EB%8A%94%EB%B2%95-feat-ll-hls-a6a0c453b21f)
&lt;details&gt;
&lt;summary&gt;HLS vs WebRTC&lt;/summary&gt;

**HLS**(HTTP Live Streaming)와 **WebRTC**는 모두 실시간 스트리밍을 위한 기술이지만, **사용 목적**, **지연 시간**, **보안** 및 **호환성** 측면에서 큰 차이가 있습니다. 사용하려는 애플리케이션의 유형에 따라 각 기술의 장단점을 비교해보고 어떤 것이 더 적합한지 선택해야 합니다. 아래는 HLS와 WebRTC를 실시간 스트리밍 웹 서비스의 관점에서 비교한 것입니다.


### 1. 지연 시간

- **HLS**:
	- **지연 시간**: HLS는 HTTP 기반 프로토콜로 설계되어 있으며, **10~30초**의 지연 시간이 발생하는 것이 일반적입니다. 심지어 **저지연 HLS(LL-HLS)**를 사용하더라도 2~5초 정도의 지연 시간이 존재할 수 있습니다.
	- **용도**: 따라서 HLS는 실시간성이 요구되지 않는 스트리밍에 적합합니다. 예를 들어, 뉴스 방송, 스포츠 경기, 강의 등의 상황에서 몇 초 정도의 지연이 큰 문제가 되지 않는다면 HLS가 적합합니다.
- **WebRTC**:
	- **지연 시간**: WebRTC는 **1초 미만의 매우 낮은 지연 시간**을 목표로 설계되어, 실시간 인터랙티브한 통신에 적합합니다.
	- **용도**: 화상 회의, 게임 스트리밍, 실시간 원격 조작 등 **매우 빠른 반응**이 필요한 애플리케이션에 적합합니다. WebRTC의 P2P 연결 방식 덕분에 지연 시간을 최소화할 수 있습니다.

### 2. 사용 사례

- **HLS**:
	- **주로 대규모 시청자 대상의 스트리밍**: HLS는 대규모 스트리밍 서비스(예: YouTube, Twitch 등)에서 매우 효과적입니다. HTTP 기반이라 기존 **CDN(Content Delivery Network)** 인프라를 활용해 손쉽게 콘텐츠를 전송할 수 있으며, 수천에서 수백만 명의 시청자가 동시에 스트리밍을 보는 데 적합합니다.
	- **적응형 스트리밍**: 네트워크 상태에 따라 비디오 품질을 조정하는 **적응형 스트리밍**을 기본적으로 제공하므로, 시청자의 네트워크 환경에 맞춰 끊김 없이 서비스를 제공합니다.
- **WebRTC**:
	- **1:1 혹은 소규모 그룹 통신**: WebRTC는 **화상 통화**, **화상 회의**, **온라인 협업 툴** 등 소규모 실시간 커뮤니케이션에 최적화되어 있습니다. 또한, 게임 스트리밍과 같은 실시간 반응이 중요한 상황에도 적합합니다.
	- **P2P 연결**: WebRTC는 주로 **P2P 연결**을 통해 데이터를 직접 전송하므로 서버의 부하를 줄일 수 있지만, 대규모 시청자를 대상으로 하기에 효율적이지 않을 수 있습니다.

### 3. 확장성

- **HLS**:
	- **대규모 스트리밍에 최적**: HLS는 HTTP 기반이기 때문에 **CDN**을 사용하여 확장성을 쉽게 확보할 수 있습니다. 수천 명 이상의 사용자가 동시에 스트리밍을 소비할 수 있으며, 서버 부담을 덜 수 있습니다.
- **WebRTC**:
	- **제한된 확장성**: WebRTC는 브라우저 간 **P2P 연결**을 사용하므로, 직접 연결의 수가 증가할수록 확장성이 제한됩니다. 예를 들어, 많은 참가자 간의 연결이 필요하면 각 클라이언트의 네트워크 및 CPU 자원이 급격히 소모됩니다.
	- **SFU 사용**: 확장성을 늘리기 위해 **SFU(Selective Forwarding Unit)**를 도입하여 각 클라이언트가 모든 참가자와 직접 연결하지 않고 중앙 서버를 통해 연결을 관리할 수 있도록 할 수 있습니다. 이를 통해 약 100명 이상의 사용자까지 확장 가능합니다.

### 4. 네트워크 호환성 및 안정성

- **HLS**:
	- **HTTP 기반 전송**: HLS는 HTTP 기반이기 때문에, 대부분의 방화벽을 통과하고 네트워크 호환성이 매우 뛰어납니다.
	- **안정적**: HTTP와 TCP를 사용하여 데이터 전송이 신뢰적이며, 중간에 발생하는 패킷 손실을 재전송하는 메커니즘이 있어 안정적인 스트리밍을 제공합니다.
- **WebRTC**:
	- **NAT Traversal**: WebRTC는 P2P 연결을 위해 **STUN** 및 **TURN** 서버를 사용하여 NAT 뒤에 있는 클라이언트를 연결합니다. 하지만 네트워크 환경에 따라 연결 설정이 복잡해지거나 문제가 발생할 수 있습니다.
	- **UDP 기반 전송**: 주로 **UDP**를 사용하여 낮은 지연 시간을 제공하지만, 패킷 손실 시 재전송을 보장하지 않아 네트워크 상태가 좋지 않을 때 품질 저하가 발생할 수 있습니다.

### 5. 보안

- **HLS**:
	- **HTTPS와 함께 사용**: HLS는 HTTP 기반으로, **HTTPS**를 사용해 데이터를 암호화할 수 있습니다. 또한, **DRM(디지털 권리 관리)**과 함께 사용해 콘텐츠 보호를 구현할 수 있습니다.
- **WebRTC**:
	- **기본적으로 암호화된 통신**: WebRTC는 모든 오디오, 비디오, 데이터 스트림을 **DTLS**(Datagram Transport Layer Security)와 **SRTP**(Secure Real-Time Transport Protocol)를 사용해 암호화합니다. 기본적으로 강력한 보안이 내장되어 있습니다.

### 6. 브라우저 지원

- **HLS**:
	- **Safari와 iOS 네이티브 지원**: Apple 기기와 Safari 브라우저에서 기본적으로 지원하지만, Chrome, Firefox 등 다른 브라우저에서는 JavaScript 라이브러리(**hls.js**)가 필요합니다.
- **WebRTC**:
	- **모든 최신 브라우저 지원**: WebRTC는 **Chrome**, **Firefox**, **Safari**, **Edge** 등 대부분의 최신 브라우저에서 네이티브로 지원됩니다. 추가적인 플러그인이 필요 없이 실시간 통신 기능을 사용할 수 있습니다.

### 결론

- **HLS**는 **대규모 스트리밍**에 적합하며, 상대적으로 긴 지연 시간을 허용할 수 있는 **방송, 교육, 엔터테인먼트**와 같은 서비스에서 주로 사용됩니다. **HTTP 기반**이므로 네트워크 호환성이 높고, 기존 CDN 인프라를 활용할 수 있는 장점이 있습니다.
- **WebRTC**는 **즉각적인 반응이 필요한 실시간 인터랙티브 애플리케이션**에 적합합니다. **낮은 지연 시간**과 **보안성**을 갖추고 있어 **화상 회의, 실시간 통신, 온라인 협업 도구** 등에 적합하며, 브라우저에서 네이티브로 지원되는 장점이 있습니다. 다만, **확장성** 측면에서는 SFU와 같은 구조적 보완이 필요합니다.

따라서, **대규모 시청자와의 방송**에는 HLS가 적합하고, **소규모 실시간 상호작용**이나 **낮은 지연 시간이 필요한 서비스**에는 WebRTC가 더 나은 선택이 될 것입니다.


&lt;/details&gt;


## 홍창현


	# WebRTC (Web Real-Time Communication)


	웹 브라우저 간에 플러그인의 도움 없이 서로 통신할 수 있도록 설계된 **Javascript API**


	→ 별다른 소프트웨어 없이 카메라, 마이크 등을 사용하여 실시간 커뮤니케이션을 제공


	음성 통화, 영상 통화, P2P 파일 공유 등으로 활용됨


	## WebRTC의 장점


	### WebRTC는 낮은 Latency를 갖는다

	- WebRTC는 **`P2P(peer-to-peer)방식`**으로 데이터를 전송
		- **P2P**는 **중간 서버**를 거치지 않음
	- WebRTC는 **UDP 기반**으로 작동하여 신속하게 패킷을 전송
		- HLS와 RTMP는 **TCP 기반**

	### WebRTC는 호환성이 높다


	![17](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/17.png)_image.png_


	## WebRTC의 단점


	### 많은 사용자가 사용할 수 없다 (스케일링 문제)

	- WebRTC는 P2P(peer-to-peer) 구조로 작동하기 때문에 각 사용자 간의 직접적인 연결을 설정
	- 사용자가 많아질수록 필요한 연결 수가 **기하급수적으로 증가**함

	![18](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/18.png)_image.png_

	- N명의 사용자가 있을 때 사용자 한명이 추가되면 N개의 연결이 필요
		- 서버와 클라이언트 모두에 큰 부담
		- 네트워크 대역폭과 성능 저하 초래

	### 화질 문제 (성능문제)

	- 실시간 비디오와 오디오 스트리밍을 위해 상당한 대역폭을 소모
	- 특히 고화질 비디오 스트림은 많은 대역폭을 요구
	- 대역폭이 제한된 환경에서는 **패킷 손실**이 발생할 수 있으며, 이는 전체 스트림의 품질 저하로 이어짐
		- WebRTC는 **UDP 기반**이므로 패킷 손실이 일어날 수 있음
	- 다수의 사용자가 동시에 스트리밍을 시도하면 **대역폭이 고갈**될 위험

	### 대규모 라이브 방송에 불완전함

	- **예측 불가능한 대역폭 소모**
		- **HLS나 RTMP**의 예측 가능한 방식
			- HLS나 RTMP는 중앙 서버에서 스트리밍을 관리하고 일반적으로 미리 인코딩된 비디오 조각을 전송
			- 스케일링 가능 : 수많은 사용자에게 콘텐츠를 효율적으로 배포할 수 있음
		- **WebRTC**는 미리 인코딩된 스트림을 사용하지 않기 때문에 대역폭이 비디오 품질과 사용자 수에 따라 즉각적으로 변동함

	## WebRTC보다 빠른 기술이 있을까?


	## **WebTransport**

	- WebTransport는 UDP 기반으로 통신
	- WebRTC의 signaling 없이도 서버와의 실시간 데이터 교환 가능

	그러나 WebTransport는 아직 개발 단계이며, 안정적인 표준화가 이루어지지 않아 모든 브라우저에서의 지원이 불확실


	WebRTC처럼 실시간 오디오와 비디오 전송을 위한 최적화가 충분하지 않음


## 김준서


	&amp;gt; 참고자료  
	&amp;gt; [https://www.cdnetworks.com/ko/blog/media-delivery/what-is-rtmp-ingest/](https://www.cdnetworks.com/ko/blog/media-delivery/what-is-rtmp-ingest/)  
	&amp;gt; [https://ossrs.net/lts/en-us/docs/v6/doc/flv](https://ossrs.net/lts/en-us/docs/v6/doc/flv)  
	&amp;gt; [https://growthvalue.tistory.com/178](https://growthvalue.tistory.com/178)  
	&amp;gt; [https://ossrs.net/lts/en-us/docs/v4/doc/delivery-http-flv](https://ossrs.net/lts/en-us/docs/v4/doc/delivery-http-flv)  
	&amp;gt; [https://medium.com/@delivalue100/rtmp-realtime-messaging-protocol-c4474e464ffd](https://medium.com/@delivalue100/rtmp-realtime-messaging-protocol-c4474e464ffd)  
	&amp;gt; [https://obsproject.com/forum/whats-new/posts/2763754/](https://obsproject.com/forum/whats-new/posts/2763754/) - OBS 포럼   
	&amp;gt; [https://devocean.sk.com/blog/techBoardDetail.do?ID=164296](https://devocean.sk.com/blog/techBoardDetail.do?ID=164296)  
	&amp;gt; [https://blog.naver.com/mingyo01/222050438291](https://blog.naver.com/mingyo01/222050438291)  
	&amp;gt; [https://velog.io/@chosj1526/%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8A%B8%EC%9C%84%EC%B9%98-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8](https://velog.io/@chosj1526/%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8A%B8%EC%9C%84%EC%B9%98-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%94%94%EC%9E%90%EC%9D%B8)


	### 용어 정리


	bitrate - 시간당 처리하는 비트의 수


	### RTMP


	어도브에서 규정한 오디오, 비디오 데이터 통신 기술을 의미한다.


	어도브에서 규정한 만큼 기존에는 Flash Player을 지원하기 위해 작성된 기술이었다.


	그러나 최근 어도브에서 Flash Player의 지원을 중단한 만큼 점차 사용률이 저조해지고 있다.


	하지만 이는 클라이언트 단에서의 문제점이고, 영상 데이터를 서버로 옮기고 저장하는 데에 있어서는 높은 지연 시간과 효율을 가지고 있기에, HLS, MPEG-DASH, HTTP-FLV와 같은 기술과 함께 사용된다.


	![19](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/19.png)_image.png_


	![20](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/20.png)_image.png_


	RTMP(Real-Time Messaging Protocol)의 지연 시간이 빠른 이유는 다음과 같습니다:

	1. **지속적인 연결 유지**: RTMP는 클라이언트와 서버 간에 **지속적인 TCP 연결**을 유지합니다. 이는 데이터 전송 시마다 새로운 연결을 설정할 필요가 없기 때문에 **연결 설정에 따른 오버헤드**를 줄여줍니다.
	2. **작은 청크(chunk) 단위 전송**: 데이터를 **작은 청크로 분할하여 전송**함으로써, 데이터가 준비되는 즉시 전송할 수 있습니다. 이는 **버퍼링 시간을 최소화**하고, 실시간 성능을 향상시킵니다.

	트위치의 경우 스트리머 → Ingest → Transcode → Replication → Edge → 시청자를 거치며 스트리밍 데이터를 전송


	![21](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/21.png)_image.png_

	- Ingest - 스트리머의 비디오 영상이 트위치 데이터 센터로 가는 것
	- Transcode - 비디오 형식을 바꾸는 것
	- Replication - 복사. 안정성을 위해
	- Edge - CDN이라고도 부름

	![22](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/22.png)_image.png_


	![23](/upload/2024-10-28-동영상_스트리밍_처리_프로토콜을_알아보자.md/23.png)_image.png_


## 김지수


## RTMP 프로토콜

[bookmark](https://bmobmo.tistory.com/16)


### **1. 영상 전송 프로토콜 종류**

- RTSP : 1996년 온라인 비디오 스트리밍용 프로토콜
- RTMP : 2009년 기술 일반인 공개
- SRT : 2017년 오픈 라이선스 전환
- NDI : 2016년 다양한 소프트웨어에 뉴텍은 무상 배포
- HLS : 애플에 의해 2009년 공개
- DASH : 2011년 공개
- WebRTC : 구글 2011년 공개

### RTSP **(Real Time Streaming Protocol)**


스트리밍의 시작이라고 말할 수 있다. 1996년 등장하였으며 RTSP가 등장 전 영상, 음악 등 멀티미디어 정보를 완전히 다운로드한 후 시청할 수 있었다.


CCTV에서 사용하고 있는 프로토콜


다만 오래된 기술이라 화질 저하, 미디어 서버 운영에 대한 높은 난이도 등으로 도태되고 있는 실정이다.


### RTMP **(Real Time Messaging Protocol)**


### **개요**

- **개발자:** Adobe Systems (원래 Macromedia가 개발)
- **출시 시기:** 2003년
- **주요 용도:** 실시간 비디오 및 오디오 스트리밍, 특히 라이브 방송
- **특징:**
	- **저지연 스트리밍:** 실시간 스트리밍에 적합한 낮은 지연 시간
	- **양방향 통신:** 클라이언트와 서버 간의 실시간 데이터 전송 가능
	- **유연성:** 비디오, 오디오, 데이터 스트림을 동시에 전송

### **한계점**

- **보안:** 기본적으로 보안 기능이 내장되어 있지 않아 데이터 암호화가 필요할 경우 추가 설정이 필요함
- **방화벽 문제:** 전용 포트(기본적으로 1935)를 사용하므로 일부 네트워크 환경에서는 차단될 수 있음
- **모바일 지원 부족:** HTTP 기반 스트리밍 프로토콜(HLS, MPEG-DASH)에 비해 모바일 기기에서의 지원이 제한적
- **확장성:** 대규모

### **SRT (Secure Reliable Transport)**


OBS 사용


### **개요**

- **개발자:** Haivision
- **출시 시기:** 2017년
- **주요 용도:** 불안정한 네트워크 환경에서도 안정적이고 보안이 강화된 비디오 스트리밍
- **특징:**
	- **보안 강화:** AES 암호화를 통해 데이터 전송 시 보안을 보장
	- **신뢰성:** 패킷 손실, 지연, 네트워크 변동성에 강한 내성을 가짐
	- **적응성:** 다양한 네트워크 조건에 맞춰 동적으로 조정
	- **오픈 소스:** SRT는 오픈 소스 프로젝트로, 다양한 플랫폼과 쉽게 통합 가능

### **SRT의 주요 기능**

- **암호화:** 전송 중인 데이터를 암호화하여 도청 및 데이터 변조를 방지
- **재전송 메커니즘:** 패킷 손실 시 재전송을 통해 데이터의 완전성을 유지
- **동적 비트레이트 조정:** 네트워크 상태에 따라 비트레이트를 자동으로 조정하여 스트리밍 품질을 최적화
- **방화벽 우회:** UDP 기반이지만, NAT 및 방화벽 환경에서도 안정적으로 동작

### **장점**

1. **보안:** RTMP와 달리 기본적으로 데이터 암호화를 지원하여 보안성이 뛰어남
2. **신뢰성:** 불안정한 네트워크 환경에서도 안정적인 데이터 전송을 보장
3. **유연성:** 다양한 네트워크 조건에 적응하여 최적의 스트리밍 품질 제공
4. **확장성:** 대규모 스트리밍 환경에서도 효율적으로 확장 가능
5. **오픈 소스:** 무료로 사용 가능하며, 커뮤니티 지원을 통해 지속적으로 개선됨

### **단점**

1. **채택률:** RTMP에 비해 상대적으로 최근에 등장한 프로토콜로, 기존 인프라와의 호환성 문제 발생 가능
2. **설정 복잡성:** 초기 설정과 최적화를 위해 기술적인 지식이 필요할 수 있음

---


## **RTMP와 SRT의 비교**


| **특징**      | **RTMP**                         | **SRT**                   |
| ----------- | -------------------------------- | ------------------------- |
| **보안**      | 기본적으로 보안 기능 없음 (RTMPS로 보안 강화 가능) | AES 암호화 내장, 기본적으로 보안 강화   |
| **전송 프로토콜** | TCP 기반                           | UDP 기반                    |
| **신뢰성**     | TCP의 신뢰성 제공, 그러나 네트워크 변동성에 취약    | 패킷 손실 복구, 네트워크 변동성에 강한 내성 |
| **지연 시간**   | 낮음                               | 낮음                        |
| **방화벽 우회**  | 전용 포트 사용, 방화벽 문제 발생 가능           | NAT 및 방화벽 환경에서도 안정적 동작    |
| **확장성**     | 대규모 분산 환경에서 추가 설정 필요             | 대규모 스트리밍 환경에서 효율적으로 확장 가능 |
| **오픈 소스**   | 아니오                              | 예 (오픈 소스 프로젝트)            |


[bookmark](https://blog.naver.com/n_cloudplatform/222493527661)


**카메라 ▶ Encoder(인코더) ▶ Media Server(+CDN Server) ▶  동영상 플레이어 ▶ 시청자(Client)**


압축되지 않은 동영상은 용량이 크기 때문에 압축하는 과정이 필요하다. 이 과정에서 코덱을 활용한다.


코덱이란?


### **카메라 ▶ Encoder(인코더)**


대표적인 코덱에서는 H.264, 음성은 AAC


웹 배포용으로 사용


원본 파일을 압축할 때, 손실압축, 무손실 압축이 존재함


JPEG - 손실 압축


PNG - 무손실 압축. 이미지 디테일 손실이 없음, 상대적으로 많은 메모리 사용


상황과 목적에 맞는 압축 방식이 필요함


### **Encoder(인코더) ▶ Media Server(+CDN Server)**


스트리밍이란, **멀티미디어 파일을 다운로드 하는 동시에 실행하는 방법이나 기술**을 말합니다. 예를 들면 넷플릭스에서 영화를 보는데 영화 전체를 다운로드 받고 난 뒤에 즉, 2GB 가량의 영상을 전부 다운 받은 후 영화가 재생된다면 사용자 불만이 폭주하겠죠?


​


실제 넷플릭스는 그렇지 않습니다. **다운로드와 동시에 바로 재생이 되며, 추가 다운로드가 계속 진행되는 방식**을 스트리밍이라고 하며, 이러한 스트리밍의 규칙을 ​**스트리밍 프로토콜**이라고 부릅니다.


**Encoder : RTMP,RTSP,webRTC,SRT**
**Player : HLS,DASH,LL-HLS,webRTC,SRT**


**Encoder**의 경우 주로 **RTMP(Real Time Messaging Protocol) 프로토콜**을 사용합니다.


​


과거에는 UDP기반의 RTSP(Real Time Streaming Protocol) 프로토콜을 많이 사용하였으나 최근에는 RTMP 프로토콜이 거의 표준이 되어가고 있습니다.


**Player**의 경우에는 **HLS(HTTP-Live Streaming), MPEG-DASH**가 대표적입니다.


​


스트리밍이란 &apos;다운로드와 동시에 미디어가 재생되는 기술&apos;이라고 설명 드렸는데, 스트리밍의 효율적인 동작을 위해서는 파일을 작은 단위로 분할해야 합니다.


​


이 과정은 대표적인 Player 프로토콜인 HLS, DASH을 통해 알아보도록 하겠습니다.


**Player 프로토콜 작동 과정 이해하기** feat. HLS &amp;amp; DASH


**✅ 우선 H.264 + AAC등 포맷의 동영상 파일을 작은 단위로 분할**​합니다.


(용량에 따라 2초 ~10초 단위)


**​**


**✅ 이와 더불어 분할된 파일의 재생순서가 작성된 manifest 파일을 생성**합니다.


manifest파일에는 분할된 동영상 파일을 어떤 순서로 몇 초간 재생할 것인지에 대한 내용들이 텍스트로 작성되어 있습니다. HLS의 경우에는 .m3u8 파일이, Dash의 경우 .mpd(xml)와 같은 manifest 파일이 생성됩니다.이러한 과정을 거쳐 작은 단위로 분할된 미디어 파일은 mp2ts와 mp4로 구성되어 있고 mp2ts의 확장자는 .ts입니다.


**​**


**✅** 마지막으로 HLS, DASH를 지원하는 브라우저나 Application에서 **Manifest 파일을 읽어서 재생**합니다. Manifest 파일에 작성되어 있는 분할된 동영상 파일을 순차적으로 읽어 들여 재생하게 됩니다. 따라서 첫번째 segment file을 다 불러오게 되면 재생을 시작할 수 있게 되고, 재생이 진행되면서 2번째 및 3번째 segment file을 뒤에서 계속 실행합니다.


​


만약 동영상을 중간부터 재생한다고 하면 manifest 파일(.m3u8, mpd)에 근거하여 해당 타임에 맞는 segment file을 먼저 다운로드를 하게 될 것입니다.


로컬 PC에 녹화하는 기능


### **Media Server(+CDN Server) ▶  동영상 플레이어**


안정적인 송출을 위해서는 충분한 인터넷 업로드, 대역폭이 확보되어야 합니다. 따라서 **Bitrate를 변환**하는 작업도 필수적으로 필요합니다.


**비트레이트(Bitrate)**는 **특정한 시간 단위(이를테면 초 단위)마다 처리하는 비트의 수**를 뜻합니다.


​


나아가 **멀티비트레이트(Multi Bitrate)**는 비디오 플레이어에서 보여지는 화질 선택 기능과 밀접하게 관련이 있습니다. 멀티비트레이트는 **비트레이트가 다른 여러 개의 영상을 준비하여 필요에 따라 영상을 전환하는 방식이나 기술**을 뜻합니다.


### **동영상 플레이어 ▶ 시청자(Client)**


이후 실시간으로 생성한 HLS 및DASH 영상 조각 파일을 사용자에게 전달하려면


**전송 서버**


가 있어야합니다.


**일반적인 미디어 서버는 전송 서버의 역할까지 수행**


하지만, 동시 시청자가 많은 방송일 경우에는


**대규모 트래픽을 안정적으로 처리하기 위해 CDN을 사용**


하는 것을 권장합니다.


[bookmark](https://medium.com/@delivalue100/rtmp-realtime-messaging-protocol-c4474e464ffd)


[bookmark](https://blog.twitch.tv/en/2023/09/28/twitch-state-of-engineering-2023/)


**트랜스코딩** 시스템은 제작자로부터 들어오는 실시간 메시징 프로토콜(RTMP) 스트림을 가져와 HLS 호환 스트림으로 변환합니다.


[bookmark](https://blog.twitch.tv/en/2022/04/26/ingesting-live-video-streams-at-global-scale/)


[bookmark](https://blog.twitch.tv/en/2021/10/25/low-latency-high-reach-creating-an-unparalleled-live-video-streaming-network-at-twitch/)


## **1. 기본 용어**


### **1.1. 스트리밍 (Streaming)**

- **설명:** 동영상이나 오디오 콘텐츠를 다운로드하지 않고 실시간으로 재생하는 기술입니다. 사용자는 데이터가 전송되는 동시에 콘텐츠를 시청할 수 있습니다.
- **예시:** 유튜브, 넷플릭스에서의 동영상 시청.

### **1.2. 버퍼링 (Buffering)**

- **설명:** 원활한 스트리밍을 위해 일시적으로 데이터를 미리 다운로드하여 저장하는 과정입니다. 네트워크 지연이나 변동성이 있을 때 재생 중단을 최소화합니다.
- **예시:** 동영상이 로딩되면서 일시정지 상태가 되는 현상.

### **1.3. 지연 시간 (Latency)**

- **설명:** 데이터가 송신지에서 수신지까지 도달하는 데 걸리는 시간입니다. 실시간 스트리밍에서는 지연 시간이 짧을수록 더 원활한 경험을 제공합니다.
- **예시:** 라이브 방송에서의 채팅 반응 속도.

---


## **2. 기술적 용어**


### **2.1. 코덱 (Codec)**

- **설명:** 비디오와 오디오 데이터를 압축하고 압축을 해제하는 소프트웨어 또는 하드웨어입니다. 효율적인 전송과 저장을 가능하게 합니다.
- **종류:**
	- **H.264 (AVC):** 널리 사용되는 비디오 코덱.
	- **H.265 (HEVC):** H.264보다 더 높은 압축 효율.
	- **VP9, AV1:** 오픈 소스 비디오 코덱.
	- **AAC, MP3:** 오디오 코덱.

### **2.2. 비트레이트 (Bitrate)**

- **설명:** 동영상이나 오디오 데이터의 전송 속도를 비트 단위로 나타낸 값입니다. 높은 비트레이트는 더 나은 품질을 제공하지만, 더 많은 대역폭을 필요로 합니다.
- **종류:**
	- **고정 비트레이트 (CBR):** 일정한 비트레이트로 전송.
	- **가변 비트레이트 (VBR):** 필요에 따라 비트레이트를 조정.

### **2.3. 해상도 (Resolution)**

- **설명:** 화면의 가로와 세로 픽셀 수를 나타내는 지표로, 동영상의 선명도와 품질을 결정합니다.
- **예시:** 1920x1080 (Full HD), 1280x720 (HD), 3840x2160 (4K).

### **2.4. 프레임 레이트 (Frame Rate)**

- **설명:** 초당 표시되는 프레임 수로, 동영상의 부드러움을 결정합니다.
- **예시:** 24fps, 30fps, 60fps.

### **2.5. 컨테이너 (Container)**

- **설명:** 비디오, 오디오, 자막 등의 다양한 미디어 데이터를 하나의 파일로 묶는 형식입니다.
- **종류:**
	- **MP4:** 가장 널리 사용되는 컨테이너.
	- **MKV:** 다양한 코덱과 기능을 지원.
	- **AVI, MOV:** 다른 일반적인 컨테이너 형식.

---


## **3. 스트리밍 프로토콜**


### **3.1. RTMP (Real-Time Messaging Protocol)**

- **설명:** Adobe에서 개발한 실시간 스트리밍 프로토콜로, 라이브 스트리밍에 주로 사용됩니다.
- **특징:** 낮은 지연 시간, 양방향 통신 지원.

### **3.2. HLS (HTTP Live Streaming)**

- **설명:** Apple에서 개발한 HTTP 기반의 스트리밍 프로토콜로, 적응형 비트레이트 스트리밍을 지원합니다.
- **특징:** HTTP 인프라 활용, 광범위한 디바이스 호환성.

### **3.3. MPEG-DASH (Dynamic Adaptive Streaming over HTTP)**

- **설명:** ISO 표준의 HTTP 기반 스트리밍 프로토콜로, 적응형 비트레이트를 지원하며 다양한 미디어 형식을 지원합니다.
- **특징:** 개방형 표준, 다양한 플랫폼과 호환.

### **3.4. WebRTC (Web Real-Time Communication)**

- **설명:** 브라우저 간 실시간 통신을 가능하게 하는 오픈 소스 프로젝트로, 주로 화상 회의 등에 사용됩니다.
- **특징:** 매우 낮은 지연 시간, P2P 연결 지원, 보안 통신.

### **3.5. SRT (Secure Reliable Transport)**

- **설명:** Haivision에서 개발한 프로토콜로, 불안정한 네트워크 환경에서도 안정적인 전송을 목표로 합니다.
- **특징:** 패킷 손실 복구, 보안 기능 강화, 네트워크 적응성.

---


## **4. 인프라 관련 용어**


### **4.1. CDN (Content Delivery Network)**

- **설명:** 전 세계에 분산된 서버 네트워크로, 사용자에게 콘텐츠를 빠르고 안정적으로 전달합니다.
- **기능:** 지리적 근접 서버 사용, 대역폭 분산, 부하 분산.

### **4.2. 인코딩 (Encoding)**

- **설명:** 원본 미디어 데이터를 특정 코덱과 설정을 사용하여 압축 및 변환하는 과정입니다.
- **목적:** 효율적인 저장과 전송을 위해 비트레이트와 해상도를 조정.

### **4.3. 트랜스코딩 (Transcoding)**

- **설명:** 이미 인코딩된 미디어 데이터를 다른 형식이나 코덱으로 변환하는 과정입니다.
- **용도:** 다양한 디바이스와 플랫폼에 맞춘 스트리밍.

### **4.4. 패키징 (Packaging)**

- **설명:** 인코딩된 미디어를 특정 스트리밍 프로토콜 형식으로 변환하는 과정입니다.
- **예시:** HLS, MPEG-DASH를 위한 세그먼트 생성.

### **4.5. 캐싱 (Caching)**

- **설명:** 자주 요청되는 데이터를 임시로 저장하여 접근 속도를 높이는 기술입니다.
- **용도:** CDN에서의 콘텐츠 빠른 전달, 버퍼링 감소.

---


## **5. 품질 및 성능 관련 용어**


### **5.1. 적응형 비트레이트 (Adaptive Bitrate)**

- **설명:** 사용자의 네트워크 조건에 따라 자동으로 비트레이트를 조정하여 최적의 재생 품질을 유지하는 기술입니다.
- **프로토콜:** HLS, MPEG-DASH.

### **5.2. 시작 지연 시간 (Start-up Latency)**

- **설명:** 스트리밍 시작부터 첫 프레임이 재생되기까지 걸리는 시간입니다.
- **중요성:** 사용자 경험에 큰 영향을 미침.

### **5.3. 캐시 히트/미스 (Cache Hit/Miss)**

- **설명:** 요청된 데이터가 캐시에 존재하는지 여부를 나타냅니다. 캐시 히트는 빠른 응답을, 미스는 원본 서버로부터 데이터를 가져와야 함을 의미합니다.

### **5.4. 지터 (Jitter)**

- **설명:** 패킷 전송 간의 시간 변동을 의미하며, 실시간 스트리밍의 품질에 영향을 미칠 수 있습니다.
- **영향:** 영상의 끊김이나 오디오의 왜곡을 유발할 수 있음.

### **5.5. 패킷 손실 (Packet Loss)**

- **설명:** 전송 중에 데이터 패킷이 손실되는 현상입니다.
- **영향:** 영상 및 오디오 품질 저하, 재생 중단.

---


## **6. 기타 관련 용어**


### **6.1. DRM (Digital Rights Management)**

- **설명:** 디지털 콘텐츠의 저작권 보호를 위한 기술 및 정책입니다.
- **용도:** 불법 복제 방지, 콘텐츠 접근 제어.

### **6.2. QoS (Quality of Service)**

- **설명:** 네트워크 성능을 관리하고 보장하기 위한 기술 및 정책입니다.
- **용도:** 스트리밍의 안정성과 품질을 유지.

### **6.3. GOP (Group of Pictures)**

- **설명:** 비디오 인코딩에서 I-프레임, P-프레임, B-프레임 등으로 구성된 프레임 그룹입니다.
- **영향:** 압축 효율과 재생 품질에 영향을 미침.

### **6.4. HDR (High Dynamic Range)**

- **설명:** 더 넓은 색역과 명암 대비를 제공하는 영상 기술입니다.
- **장점:** 더 생동감 있고 현실적인 영상 표현.

### **6.5. LUT (Look-Up Table)**

- **설명:** 색 보정과 그레이딩을 위해 사용되는 표로, 색상 변환을 빠르게 적용할 수 있습니다.
- **용도:** 비디오 후반 작업에서 색상 일관성 유지.

### **6.6. 비디오 월 (Video Wall)**

- **설명:** 여러 대의 디스플레이를 연결하여 하나의 큰 화면을 구성하는 시스템입니다.
- **용도:** 대형 이벤트, 컨트롤 룸, 광고 등에서 사용.

### **6.7. OTT (Over-The-Top)**

- **설명:** 인터넷을 통해 제공되는 미디어 서비스로, 전통적인 방송 플랫폼을 거치지 않습니다.
- **예시:** 넷플릭스, 디즈니+, 아마존 프라임 비디오.

### **6.8. VOD (Video on Demand)**

- **설명:** 사용자가 원하는 시간에 원하는 콘텐츠를 시청할 수 있는 서비스입니다.
- **예시:** 넷플릭스, 유튜브의 프리미엄 서비스.

### **6.9. CDN (Content Delivery Network)**

- **설명:** 전 세계에 분산된 서버 네트워크로, 콘텐츠를 사용자에게 빠르고 효율적으로 전달합니다.
- **예시:** Akamai, Cloudflare, Amazon CloudFront.

### **6.10. 클라우드 인코딩 (Cloud Encoding)**

- **설명:** 클라우드 기반 서비스에서 비디오를 인코딩하는 프로세스입니다.
- **장점:** 확장성, 유연성, 비용 효율성.

## **1. Codec의 어원 (Etymology)**


&quot;Codec&quot;은 두 단어의 합성어입니다:

- **CO**der (인코더): 데이터를 특정 형식으로 변환하거나 압축하는 장치 또는 소프트웨어.
- **DE**coder (디코더): 인코딩된 데이터를 원래 형식으로 복원하거나 해제하는 장치 또는 소프트웨어.

따라서, &quot;Codec&quot;은 &quot;Coder&quot;와 &quot;Decoder&quot;의 결합으로 이루어진 단어입니다. 이 합성어는 1980년대 초반에 처음 등장했으며, 디지털 오디오와 비디오 데이터를 효율적으로 전송하고 저장하기 위해 개발된 기술을 지칭하기 위해 사용되었습니다.


&amp;lt;/details&amp;gt;

&lt;/video&gt;&lt;/video&gt;&lt;/details&gt;</content>

      
      
      
      
      

      <author>
          <name>[&quot;gominzip&quot;, &quot;hoeeeeeh&quot;, &quot;Jisukim&quot;, &quot;홍창현&quot;, &quot;김준서&quot;]</name>
        
        
      </author>

      

      

      
        <summary type="html">목차</summary>
      

      
      
    </entry>
  
</feed>
